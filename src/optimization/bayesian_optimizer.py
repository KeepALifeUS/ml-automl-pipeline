"""
Bayesian Hyperparameter Optimization for Crypto Trading AutoML
Implements enterprise patterns for robust optimization
"""

import logging
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
import xgboost as xgb
import lightgbm as lgb
from skopt import gp_minimize, forest_minimize, gbrt_minimize
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
from skopt.plots import plot_convergence, plot_objective, plot_evaluations
import optuna
from loguru import logger
from pydantic import BaseModel, Field
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import time

from ..utils.config_manager import AutoMLConfig


class OptimizationMethod(Enum):
    """–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    GAUSSIAN_PROCESS = "gaussian_process"
    RANDOM_FOREST = "random_forest" 
    GRADIENT_BOOSTING = "gradient_boosting"
    OPTUNA_TPE = "optuna_tpe"
    OPTUNA_RANDOM = "optuna_random"


@dataclass
class OptimizationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    best_params: Dict[str, Any]
    best_score: float
    optimization_history: List[Dict[str, Any]]
    convergence_data: Dict[str, Any]
    optimization_time: float
    method_used: str
    model_name: str


class BaseOptimizer(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ -  pattern"""
    
    @abstractmethod
    def optimize(
        self,
        objective_function: Callable,
        search_space: Dict[str, Any],
        n_calls: int = 100
    ) -> OptimizationResult:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"""
        pass


class SkoptBayesianOptimizer(BaseOptimizer):
    """–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ scikit-optimize"""
    
    def __init__(self, method: OptimizationMethod = OptimizationMethod.GAUSSIAN_PROCESS):
        self.method = method
        self.optimization_history = []
        
    def optimize(
        self,
        objective_function: Callable,
        search_space: Dict[str, Any],
        n_calls: int = 100,
        random_state: int = 42
    ) -> OptimizationResult:
        """–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å scikit-optimize"""
        start_time = time.time()
        
        logger.info(f"üéØ –ó–∞–ø—É—Å–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–º {self.method.value}")
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
            dimensions = self._convert_search_space(search_space)
            
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
            @use_named_args(dimensions)
            def wrapped_objective(**params):
                score = objective_function(params)
                self.optimization_history.append({'params': params.copy(), 'score': score})
                return score  # scikit-optimize –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç, –ø–æ—ç—Ç–æ–º—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            
            # –í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if self.method == OptimizationMethod.GAUSSIAN_PROCESS:
                result = gp_minimize(
                    func=wrapped_objective,
                    dimensions=dimensions,
                    n_calls=n_calls,
                    random_state=random_state,
                    acq_func='EI'  # Expected Improvement
                )
            elif self.method == OptimizationMethod.RANDOM_FOREST:
                result = forest_minimize(
                    func=wrapped_objective,
                    dimensions=dimensions,
                    n_calls=n_calls,
                    random_state=random_state
                )
            else:  # GRADIENT_BOOSTING
                result = gbrt_minimize(
                    func=wrapped_objective,
                    dimensions=dimensions,
                    n_calls=n_calls,
                    random_state=random_state
                )
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            best_params = {}
            for i, dim in enumerate(dimensions):
                best_params[dim.name] = result.x[i]
            
            optimization_time = time.time() - start_time
            
            optimization_result = OptimizationResult(
                best_params=best_params,
                best_score=result.fun,
                optimization_history=self.optimization_history,
                convergence_data={
                    'func_vals': result.func_vals.tolist(),
                    'x_iters': [x.tolist() if isinstance(x, np.ndarray) else x for x in result.x_iters],
                    'n_calls': n_calls,
                    'convergence_rate': self._calculate_convergence_rate(result.func_vals)
                },
                optimization_time=optimization_time,
                method_used=self.method.value,
                model_name="unknown"
            )
            
            logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ª—É—á—à–∏–π —Å–∫–æ—Ä {result.fun:.4f}")
            return optimization_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return OptimizationResult(
                best_params={},
                best_score=float('inf'),
                optimization_history=[],
                convergence_data={},
                optimization_time=time.time() - start_time,
                method_used=f"{self.method.value}_failed",
                model_name="unknown"
            )
    
    def _convert_search_space(self, search_space: Dict[str, Any]) -> List:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç scikit-optimize"""
        dimensions = []
        
        for param_name, param_config in search_space.items():
            if param_config['type'] == 'real':
                dimensions.append(Real(
                    low=param_config['low'],
                    high=param_config['high'],
                    prior=param_config.get('prior', 'uniform'),
                    name=param_name
                ))
            elif param_config['type'] == 'integer':
                dimensions.append(Integer(
                    low=param_config['low'],
                    high=param_config['high'],
                    name=param_name
                ))
            elif param_config['type'] == 'categorical':
                dimensions.append(Categorical(
                    categories=param_config['categories'],
                    name=param_name
                ))
        
        return dimensions
    
    def _calculate_convergence_rate(self, func_vals: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        if len(func_vals) < 2:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
        improvements = []
        best_so_far = func_vals[0]
        
        for val in func_vals[1:]:
            if val < best_so_far:
                improvement = (best_so_far - val) / abs(best_so_far) if best_so_far != 0 else 0
                improvements.append(improvement)
                best_so_far = val
            else:
                improvements.append(0.0)
        
        return np.mean(improvements) if improvements else 0.0


class OptunaBayesianOptimizer(BaseOptimizer):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Optuna"""
    
    def __init__(self, method: OptimizationMethod = OptimizationMethod.OPTUNA_TPE):
        self.method = method
        self.study = None
        
    def optimize(
        self,
        objective_function: Callable,
        search_space: Dict[str, Any],
        n_calls: int = 100,
        random_state: int = 42
    ) -> OptimizationResult:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å Optuna"""
        start_time = time.time()
        
        logger.info(f"üî• –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Optuna –º–µ—Ç–æ–¥–æ–º {self.method.value}")
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            if self.method == OptimizationMethod.OPTUNA_TPE:
                sampler = optuna.samplers.TPESampler(seed=random_state)
            else:  # OPTUNA_RANDOM
                sampler = optuna.samplers.RandomSampler(seed=random_state)
            
            self.study = optuna.create_study(
                direction='minimize',
                sampler=sampler,
                study_name=f"automl_optimization_{int(time.time())}"
            )
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è Optuna
            def optuna_objective(trial):
                params = {}
                for param_name, param_config in search_space.items():
                    if param_config['type'] == 'real':
                        params[param_name] = trial.suggest_float(
                            param_name,
                            param_config['low'],
                            param_config['high'],
                            log=param_config.get('log', False)
                        )
                    elif param_config['type'] == 'integer':
                        params[param_name] = trial.suggest_int(
                            param_name,
                            param_config['low'],
                            param_config['high'],
                            log=param_config.get('log', False)
                        )
                    elif param_config['type'] == 'categorical':
                        params[param_name] = trial.suggest_categorical(
                            param_name,
                            param_config['categories']
                        )
                
                return objective_function(params)
            
            # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            self.study.optimize(optuna_objective, n_trials=n_calls, show_progress_bar=True)
            
            # –°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            optimization_history = []
            func_vals = []
            
            for trial in self.study.trials:
                if trial.state == optuna.trial.TrialState.COMPLETE:
                    optimization_history.append({
                        'params': trial.params.copy(),
                        'score': trial.value,
                        'trial_number': trial.number,
                        'duration': trial.duration.total_seconds() if trial.duration else 0
                    })
                    func_vals.append(trial.value)
            
            optimization_time = time.time() - start_time
            
            optimization_result = OptimizationResult(
                best_params=self.study.best_params.copy(),
                best_score=self.study.best_value,
                optimization_history=optimization_history,
                convergence_data={
                    'func_vals': func_vals,
                    'n_calls': len(self.study.trials),
                    'n_complete_trials': len([t for t in self.study.trials if t.state == optuna.trial.TrialState.COMPLETE]),
                    'convergence_rate': self._calculate_optuna_convergence_rate(func_vals)
                },
                optimization_time=optimization_time,
                method_used=self.method.value,
                model_name="unknown"
            )
            
            logger.info(f"‚úÖ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ª—É—á—à–∏–π —Å–∫–æ—Ä {self.study.best_value:.4f}")
            return optimization_result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return OptimizationResult(
                best_params={},
                best_score=float('inf'),
                optimization_history=[],
                convergence_data={},
                optimization_time=time.time() - start_time,
                method_used=f"{self.method.value}_failed",
                model_name="unknown"
            )
    
    def _calculate_optuna_convergence_rate(self, func_vals: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–ª—è Optuna"""
        if len(func_vals) < 2:
            return 0.0
        
        improvements = []
        best_so_far = func_vals[0]
        
        for val in func_vals[1:]:
            if val < best_so_far:
                improvement = (best_so_far - val) / abs(best_so_far) if best_so_far != 0 else 0
                improvements.append(improvement)
                best_so_far = val
            else:
                improvements.append(0.0)
        
        return np.mean(improvements) if improvements else 0.0


class CryptoMLHyperparameterOptimizer:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–µ
    –†–µ–∞–ª–∏–∑—É–µ—Ç enterprise patterns
    """
    
    def __init__(self, config: Optional[AutoMLConfig] = None):
        self.config = config or AutoMLConfig()
        self.optimizers: Dict[str, BaseOptimizer] = {}
        self.model_search_spaces = {}
        self._setup_optimizers()
        self._setup_search_spaces()
        
    def _setup_optimizers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤"""
        logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤...")
        
        # Scikit-optimize –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
        self.optimizers['gaussian_process'] = SkoptBayesianOptimizer(
            OptimizationMethod.GAUSSIAN_PROCESS
        )
        self.optimizers['random_forest'] = SkoptBayesianOptimizer(
            OptimizationMethod.RANDOM_FOREST
        )
        self.optimizers['gradient_boosting'] = SkoptBayesianOptimizer(
            OptimizationMethod.GRADIENT_BOOSTING
        )
        
        # Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
        self.optimizers['optuna_tpe'] = OptunaBayesianOptimizer(
            OptimizationMethod.OPTUNA_TPE
        )
        self.optimizers['optuna_random'] = OptunaBayesianOptimizer(
            OptimizationMethod.OPTUNA_RANDOM
        )
        
        logger.info(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ {len(self.optimizers)} –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤")
    
    def _setup_search_spaces(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üåê –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –ø–æ–∏—Å–∫–∞...")
        
        # Random Forest
        self.model_search_spaces['random_forest'] = {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 30},
            'min_samples_split': {'type': 'integer', 'low': 2, 'high': 20},
            'min_samples_leaf': {'type': 'integer', 'low': 1, 'high': 10},
            'max_features': {'type': 'categorical', 'categories': ['auto', 'sqrt', 'log2']},
        }
        
        # XGBoost
        self.model_search_spaces['xgboost'] = {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 15},
            'learning_rate': {'type': 'real', 'low': 0.01, 'high': 0.3, 'log': True},
            'subsample': {'type': 'real', 'low': 0.5, 'high': 1.0},
            'colsample_bytree': {'type': 'real', 'low': 0.5, 'high': 1.0},
            'reg_alpha': {'type': 'real', 'low': 0.0, 'high': 10.0},
            'reg_lambda': {'type': 'real', 'low': 0.0, 'high': 10.0},
        }
        
        # LightGBM
        self.model_search_spaces['lightgbm'] = {
            'n_estimators': {'type': 'integer', 'low': 50, 'high': 500},
            'max_depth': {'type': 'integer', 'low': 3, 'high': 15},
            'learning_rate': {'type': 'real', 'low': 0.01, 'high': 0.3, 'log': True},
            'subsample': {'type': 'real', 'low': 0.5, 'high': 1.0},
            'colsample_bytree': {'type': 'real', 'low': 0.5, 'high': 1.0},
            'reg_alpha': {'type': 'real', 'low': 0.0, 'high': 10.0},
            'reg_lambda': {'type': 'real', 'low': 0.0, 'high': 10.0},
            'num_leaves': {'type': 'integer', 'low': 20, 'high': 200},
        }
        
        # Ridge Regression
        self.model_search_spaces['ridge'] = {
            'alpha': {'type': 'real', 'low': 0.001, 'high': 100.0, 'log': True},
        }
        
        # Lasso Regression
        self.model_search_spaces['lasso'] = {
            'alpha': {'type': 'real', 'low': 0.001, 'high': 100.0, 'log': True},
        }
        
        # ElasticNet
        self.model_search_spaces['elasticnet'] = {
            'alpha': {'type': 'real', 'low': 0.001, 'high': 100.0, 'log': True},
            'l1_ratio': {'type': 'real', 'low': 0.0, 'high': 1.0},
        }
        
        # SVR
        self.model_search_spaces['svr'] = {
            'C': {'type': 'real', 'low': 0.1, 'high': 1000.0, 'log': True},
            'gamma': {'type': 'categorical', 'categories': ['scale', 'auto']},
            'epsilon': {'type': 'real', 'low': 0.01, 'high': 1.0},
        }
        
        logger.info(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è {len(self.model_search_spaces)} –º–æ–¥–µ–ª–µ–π")
    
    def _get_model(self, model_name: str, params: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        if model_name == 'random_forest':
            return RandomForestRegressor(**params, random_state=42, n_jobs=-1)
        elif model_name == 'xgboost':
            return xgb.XGBRegressor(**params, random_state=42, n_jobs=-1)
        elif model_name == 'lightgbm':
            return lgb.LGBMRegressor(**params, random_state=42, n_jobs=-1, verbose=-1)
        elif model_name == 'ridge':
            return Ridge(**params)
        elif model_name == 'lasso':
            return Lasso(**params, max_iter=2000)
        elif model_name == 'elasticnet':
            return ElasticNet(**params, max_iter=2000)
        elif model_name == 'svr':
            return SVR(**params)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}")
    
    def optimize_model(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        model_name: str,
        optimizer_method: str = 'optuna_tpe',
        n_calls: int = 100,
        cv_folds: int = 5,
        scoring: str = 'neg_mean_squared_error',
        time_series_split: bool = True
    ) -> OptimizationResult:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            optimizer_method: –ú–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            n_calls: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            scoring: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            time_series_split: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TimeSeriesSplit
        """
        logger.info(f"üéØ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_name}")
        
        if model_name not in self.model_search_spaces:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        
        if optimizer_method not in self.optimizers:
            raise ValueError(f"–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä {optimizer_method} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if time_series_split:
            cv = TimeSeriesSplit(n_splits=cv_folds)
        else:
            cv = cv_folds
        
        # –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        def objective_function(params: Dict[str, Any]) -> float:
            try:
                # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                model = self._get_model(model_name, params)
                
                # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                scores = cross_val_score(
                    model, X, y,
                    cv=cv,
                    scoring=scoring,
                    n_jobs=-1
                )
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏
                return -np.mean(scores)
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
                return float('inf')  # –ü–ª–æ—Ö–æ–π —Å–∫–æ—Ä –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        
        # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        search_space = self.model_search_spaces[model_name]
        optimizer = self.optimizers[optimizer_method]
        
        result = optimizer.optimize(
            objective_function=objective_function,
            search_space=search_space,
            n_calls=n_calls
        )
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        result.model_name = model_name
        
        logger.info(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ª—É—á—à–∏–π —Å–∫–æ—Ä {-result.best_score:.4f}")
        
        return result
    
    def optimize_multiple_models(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        models: List[str],
        optimizer_method: str = 'optuna_tpe',
        n_calls: int = 50,
        parallel: bool = False
    ) -> Dict[str, OptimizationResult]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info(f"üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {len(models)} –º–æ–¥–µ–ª–µ–π...")
        
        results = {}
        
        if parallel:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏)
            from joblib import Parallel, delayed
            
            def optimize_single_model(model_name):
                return model_name, self.optimize_model(
                    X, y, model_name, optimizer_method, n_calls
                )
            
            parallel_results = Parallel(n_jobs=-1, verbose=1)(
                delayed(optimize_single_model)(model) for model in models
            )
            
            for model_name, result in parallel_results:
                results[model_name] = result
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
            ) as progress:
                task = progress.add_task("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π...", total=len(models))
                
                for model_name in models:
                    progress.update(task, description=f"–ú–æ–¥–µ–ª—å: {model_name}")
                    
                    try:
                        result = self.optimize_model(
                            X, y, model_name, optimizer_method, n_calls
                        )
                        results[model_name] = result
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ {model_name}: {e}")
                    
                    progress.advance(task)
        
        logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è {len(results)} –º–æ–¥–µ–ª–µ–π")
        
        return results
    
    def plot_optimization_history(
        self,
        result: OptimizationResult,
        save_path: Optional[str] = None
    ):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if 'func_vals' in result.convergence_data:
                func_vals = result.convergence_data['func_vals']
                axes[0, 0].plot(func_vals)
                axes[0, 0].set_title('–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏')
                axes[0, 0].set_xlabel('–ò—Ç–µ—Ä–∞—Ü–∏—è')
                axes[0, 0].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏')
                axes[0, 0].grid(True)
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤
            if result.optimization_history:
                scores = [h['score'] for h in result.optimization_history]
                axes[0, 1].hist(scores, bins=20, alpha=0.7)
                axes[0, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤')
                axes[0, 1].set_xlabel('–°–∫–æ—Ä')
                axes[0, 1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                axes[0, 1].grid(True)
            
            # –£–ª—É—á—à–µ–Ω–∏—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
            if result.optimization_history:
                scores = [h['score'] for h in result.optimization_history]
                best_scores = []
                best_so_far = float('inf')
                
                for score in scores:
                    if score < best_so_far:
                        best_so_far = score
                    best_scores.append(best_so_far)
                
                axes[1, 0].plot(best_scores)
                axes[1, 0].set_title('–õ—É—á—à–∏–π —Å–∫–æ—Ä —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º')
                axes[1, 0].set_xlabel('–ò—Ç–µ—Ä–∞—Ü–∏—è')
                axes[1, 0].set_ylabel('–õ—É—á—à–∏–π —Å–∫–æ—Ä')
                axes[1, 0].grid(True)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            stats_text = f"""
            –ú–æ–¥–µ–ª—å: {result.model_name}
            –ú–µ—Ç–æ–¥: {result.method_used}
            –í—Ä–µ–º—è: {result.optimization_time:.2f}—Å
            –õ—É—á—à–∏–π —Å–∫–æ—Ä: {result.best_score:.4f}
            –ò—Ç–µ—Ä–∞—Ü–∏–π: {len(result.optimization_history)}
            """
            
            axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')
            axes[1, 1].set_xlim(0, 1)
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].axis('off')
            axes[1, 1].set_title('–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏')
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
    
    def get_optimization_report(self, results: Dict[str, OptimizationResult]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        report = "=== –û–¢–ß–ï–¢ –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í ===\n\n"
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –ø–æ –ª—É—á—à–µ–º—É —Å–∫–æ—Ä—É
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1].best_score
        )
        
        for i, (model_name, result) in enumerate(sorted_results, 1):
            report += f"{i}. {model_name.upper()}\n"
            report += f"   –õ—É—á—à–∏–π —Å–∫–æ—Ä: {result.best_score:.4f}\n"
            report += f"   –í—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {result.optimization_time:.2f}—Å\n"
            report += f"   –ú–µ—Ç–æ–¥: {result.method_used}\n"
            report += f"   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n"
            
            for param, value in result.best_params.items():
                report += f"     {param}: {value}\n"
            
            report += "\n"
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = sum(r.optimization_time for r in results.values())
        best_overall = min(results.values(), key=lambda x: x.best_score)
        
        report += f"–û–±—â–µ–µ –≤—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {total_time:.2f}—Å\n"
        report += f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_overall.model_name} (—Å–∫–æ—Ä: {best_overall.best_score:.4f})\n"
        
        return report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    from ..utils.config_manager import AutoMLConfig
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å—é
    y = pd.Series(
        X.iloc[:, :5].sum(axis=1) + 
        0.5 * X['feature_0'] * X['feature_1'] + 
        0.1 * np.random.randn(n_samples)
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    config = AutoMLConfig()
    optimizer = CryptoMLHyperparameterOptimizer(config)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
    result = optimizer.optimize_model(
        X, y, 
        model_name='xgboost',
        optimizer_method='optuna_tpe',
        n_calls=20  # –ú–∞–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    )
    
    print("=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò ===")
    print(f"–ú–æ–¥–µ–ª—å: {result.model_name}")
    print(f"–õ—É—á—à–∏–π —Å–∫–æ—Ä: {result.best_score:.4f}")
    print(f"–í—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {result.optimization_time:.2f}—Å")
    print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {result.best_params}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
    models = ['random_forest', 'xgboost']
    results = optimizer.optimize_multiple_models(X, y, models, n_calls=10)
    
    print("\n" + optimizer.get_optimization_report(results))