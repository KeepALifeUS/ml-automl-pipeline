"""
Advanced Model Selection for Crypto Trading AutoML
Implements enterprise patterns for robust model selection
"""

import logging
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, TimeSeriesSplit, StratifiedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, f1_score
from sklearn.ensemble import (
    RandomForestRegressor, RandomForestClassifier,
    GradientBoostingRegressor, GradientBoostingClassifier,
    AdaBoostRegressor, AdaBoostClassifier,
    ExtraTreesRegressor, ExtraTreesClassifier
)
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet,
    LogisticRegression, SGDRegressor, SGDClassifier
)
from sklearn.svm import SVR, SVC
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table
from rich.console import Console
import matplotlib.pyplot as plt
import seaborn as sns
import time

from ..utils.config_manager import AutoMLConfig


class TaskType(Enum):
    """–¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    REGRESSION = "regression"
    BINARY_CLASSIFICATION = "binary_classification"
    MULTICLASS_CLASSIFICATION = "multiclass_classification"


@dataclass
class ModelSelectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
    model_scores: Dict[str, float]
    best_models: List[str]
    evaluation_metadata: Dict[str, Any]
    task_type: str
    selection_time: float
    cv_results: Dict[str, List[float]]


class BaseModelProvider(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π -  pattern"""
    
    @abstractmethod
    def get_models(self, task_type: TaskType) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á–∏"""
        pass
    
    @abstractmethod
    def get_default_params(self, model_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –º–æ–¥–µ–ª–∏"""
        pass


class SklearnModelProvider(BaseModelProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–¥–µ–ª–µ–π scikit-learn"""
    
    def get_models(self, task_type: TaskType) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ scikit-learn"""
        if task_type == TaskType.REGRESSION:
            return {
                'linear_regression': LinearRegression(),
                'ridge': Ridge(random_state=42),
                'lasso': Lasso(random_state=42, max_iter=2000),
                'elasticnet': ElasticNet(random_state=42, max_iter=2000),
                'random_forest': RandomForestRegressor(random_state=42, n_jobs=-1),
                'gradient_boosting': GradientBoostingRegressor(random_state=42),
                'extra_trees': ExtraTreesRegressor(random_state=42, n_jobs=-1),
                'decision_tree': DecisionTreeRegressor(random_state=42),
                'knn': KNeighborsRegressor(n_jobs=-1),
                'svr': SVR(),
                'mlp': MLPRegressor(random_state=42, max_iter=500)
            }
        else:  # Classification
            return {
                'logistic_regression': LogisticRegression(random_state=42, max_iter=2000),
                'random_forest': RandomForestClassifier(random_state=42, n_jobs=-1),
                'gradient_boosting': GradientBoostingClassifier(random_state=42),
                'extra_trees': ExtraTreesClassifier(random_state=42, n_jobs=-1),
                'decision_tree': DecisionTreeClassifier(random_state=42),
                'knn': KNeighborsClassifier(n_jobs=-1),
                'svc': SVC(random_state=42, probability=True),
                'mlp': MLPClassifier(random_state=42, max_iter=500)
            }
    
    def get_default_params(self, model_name: str) -> Dict[str, Any]:
        """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è sklearn –º–æ–¥–µ–ª–µ–π"""
        default_params = {
            'random_forest': {'n_estimators': 100, 'max_depth': 10},
            'gradient_boosting': {'n_estimators': 100, 'max_depth': 6},
            'extra_trees': {'n_estimators': 100, 'max_depth': 10},
            'ridge': {'alpha': 1.0},
            'lasso': {'alpha': 1.0},
            'elasticnet': {'alpha': 1.0, 'l1_ratio': 0.5},
            'knn': {'n_neighbors': 5},
            'mlp': {'hidden_layer_sizes': (100,), 'alpha': 0.001}
        }
        return default_params.get(model_name, {})


class GradientBoostingModelProvider(BaseModelProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –º–æ–¥–µ–ª–µ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞"""
    
    def get_models(self, task_type: TaskType) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞"""
        if task_type == TaskType.REGRESSION:
            return {
                'xgboost': xgb.XGBRegressor(
                    random_state=42, n_jobs=-1, verbosity=0
                ),
                'lightgbm': lgb.LGBMRegressor(
                    random_state=42, n_jobs=-1, verbose=-1
                ),
                'catboost': cb.CatBoostRegressor(
                    random_state=42, verbose=False
                )
            }
        else:  # Classification
            return {
                'xgboost': xgb.XGBClassifier(
                    random_state=42, n_jobs=-1, verbosity=0
                ),
                'lightgbm': lgb.LGBMClassifier(
                    random_state=42, n_jobs=-1, verbose=-1
                ),
                'catboost': cb.CatBoostClassifier(
                    random_state=42, verbose=False
                )
            }
    
    def get_default_params(self, model_name: str) -> Dict[str, Any]:
        """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞"""
        default_params = {
            'xgboost': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            },
            'lightgbm': {
                'n_estimators': 100,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            },
            'catboost': {
                'iterations': 100,
                'depth': 6,
                'learning_rate': 0.1
            }
        }
        return default_params.get(model_name, {})


class ModelSelector:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    –†–µ–∞–ª–∏–∑—É–µ—Ç enterprise patterns
    """
    
    def __init__(self, config: Optional[AutoMLConfig] = None):
        self.config = config or AutoMLConfig()
        self.model_providers: Dict[str, BaseModelProvider] = {}
        self.console = Console()
        self._setup_providers()
        
    def _setup_providers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π"""
        logger.info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π...")
        
        self.model_providers['sklearn'] = SklearnModelProvider()
        self.model_providers['gradient_boosting'] = GradientBoostingModelProvider()
        
        logger.info(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ {len(self.model_providers)} –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤")
    
    def _detect_task_type(self, y: pd.Series) -> TaskType:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        unique_values = y.nunique()
        
        if y.dtype in ['float64', 'float32'] or unique_values > 20:
            return TaskType.REGRESSION
        elif unique_values == 2:
            return TaskType.BINARY_CLASSIFICATION
        else:
            return TaskType.MULTICLASS_CLASSIFICATION
    
    def _get_scoring_metric(self, task_type: TaskType) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∫–æ—Ä–∏–Ω–≥–∞"""
        if task_type == TaskType.REGRESSION:
            return 'neg_mean_squared_error'
        elif task_type == TaskType.BINARY_CLASSIFICATION:
            return 'f1'
        else:  # Multiclass
            return 'f1_macro'
    
    def _get_all_models(self, task_type: TaskType, models: Optional[List[str]] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        all_models = {}
        
        for provider_name, provider in self.model_providers.items():
            provider_models = provider.get_models(task_type)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ
            if models:
                provider_models = {
                    name: model for name, model in provider_models.items()
                    if name in models
                }
            
            all_models.update(provider_models)
        
        return all_models
    
    def select_best_models(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        models: Optional[List[str]] = None,
        cv_folds: int = 5,
        scoring: Optional[str] = None,
        time_series_split: bool = True,
        top_k: int = 5
    ) -> ModelSelectionResult:
        """
        –û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            X: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            y: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            cv_folds: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            scoring: –ú–µ—Ç—Ä–∏–∫–∞ —Å–∫–æ—Ä–∏–Ω–≥–∞
            time_series_split: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        """
        start_time = time.time()
        
        logger.info("ü§ñ –ó–∞–ø—É—Å–∫ –æ—Ç–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        task_type = self._detect_task_type(y)
        logger.info(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type.value}")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–∫–æ—Ä–∏–Ω–≥–∞
        if scoring is None:
            scoring = self._get_scoring_metric(task_type)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        all_models = self._get_all_models(task_type, models)
        
        if not all_models:
            logger.error("‚ùå –ù–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            return ModelSelectionResult(
                model_scores={},
                best_models=[],
                evaluation_metadata={'error': 'No models available'},
                task_type=task_type.value,
                selection_time=0.0,
                cv_results={}
            )
        
        logger.info(f"üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(all_models)} –º–æ–¥–µ–ª–µ–π...")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if time_series_split and task_type == TaskType.REGRESSION:
            cv = TimeSeriesSplit(n_splits=cv_folds)
            cv_name = f"TimeSeriesSplit({cv_folds})"
        elif task_type != TaskType.REGRESSION:
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            cv_name = f"StratifiedKFold({cv_folds})"
        else:
            cv = cv_folds
            cv_name = f"KFold({cv_folds})"
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
        model_scores = {}
        cv_results = {}
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        ) as progress:
            
            task = progress.add_task("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π...", total=len(all_models))
            
            for model_name, model in all_models.items():
                progress.update(task, description=f"–ú–æ–¥–µ–ª—å: {model_name}")
                
                try:
                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    X_clean = X.fillna(0).replace([np.inf, -np.inf], 0)
                    y_clean = y.fillna(y.mean()) if y.isna().any() else y
                    
                    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                    scores = cross_val_score(
                        model, X_clean, y_clean,
                        cv=cv,
                        scoring=scoring,
                        n_jobs=-1,
                        error_score='raise'
                    )
                    
                    mean_score = np.mean(scores)
                    model_scores[model_name] = mean_score
                    cv_results[model_name] = scores.tolist()
                    
                    logger.debug(f"‚úÖ {model_name}: {mean_score:.4f} ¬± {np.std(scores):.4f}")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                    model_scores[model_name] = float('-inf')  # –ü–ª–æ—Ö–æ–π —Å–∫–æ—Ä
                    cv_results[model_name] = []
                
                progress.advance(task)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –ø–æ —Å–∫–æ—Ä—É
        sorted_models = sorted(
            model_scores.items(),
            key=lambda x: x[1],
            reverse=True  # –î–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫ –±–æ–ª—å—à–µ = –ª—É—á—à–µ
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫ –≥–¥–µ –º–µ–Ω—å—à–µ = –ª—É—á—à–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, MSE)
        if scoring.startswith('neg_'):
            sorted_models = sorted(
                model_scores.items(),
                key=lambda x: -x[1],  # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è neg_ –º–µ—Ç—Ä–∏–∫
                reverse=True
            )
        
        best_models = [model[0] for model in sorted_models[:top_k]]
        
        selection_time = time.time() - start_time
        
        result = ModelSelectionResult(
            model_scores=model_scores,
            best_models=best_models,
            evaluation_metadata={
                'task_type': task_type.value,
                'scoring_metric': scoring,
                'cv_strategy': cv_name,
                'models_tested': len(all_models),
                'successful_models': len([s for s in model_scores.values() if s != float('-inf')])
            },
            task_type=task_type.value,
            selection_time=selection_time,
            cv_results=cv_results
        )
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_results(result)
        
        logger.info(f"‚úÖ –û—Ç–±–æ—Ä –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {selection_time:.2f}—Å")
        
        return result
    
    def _print_results(self, result: ModelSelectionResult):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        table = Table(title="üèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¢–ë–û–†–ê –ú–û–î–ï–õ–ï–ô")
        
        table.add_column("–†–∞–Ω–≥", style="cyan", no_wrap=True)
        table.add_column("–ú–æ–¥–µ–ª—å", style="magenta")
        table.add_column("–°–∫–æ—Ä", style="green")
        table.add_column("Std", style="yellow")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–∫–æ—Ä—É
        sorted_models = sorted(
            result.model_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        for i, (model_name, score) in enumerate(sorted_models[:10], 1):
            if model_name in result.cv_results and result.cv_results[model_name]:
                std_score = np.std(result.cv_results[model_name])
                std_str = f"¬±{std_score:.4f}"
            else:
                std_str = "N/A"
            
            table.add_row(
                str(i),
                model_name,
                f"{score:.4f}",
                std_str
            )
        
        self.console.print(table)
    
    def plot_model_comparison(
        self,
        result: ModelSelectionResult,
        top_n: int = 10,
        save_path: Optional[str] = None
    ):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        try:
            # –¢–æ–ø N –º–æ–¥–µ–ª–µ–π
            sorted_models = sorted(
                result.model_scores.items(),
                key=lambda x: x[1],
                reverse=True
            )[:top_n]
            
            models, scores = zip(*sorted_models)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Å–∫–æ—Ä–æ–≤
            plt.figure(figsize=(12, 8))
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
            plt.subplot(2, 1, 1)
            bars = plt.barh(models, scores, color='skyblue', alpha=0.7)
            plt.xlabel('–°–∫–æ—Ä –º–æ–¥–µ–ª–∏')
            plt.title(f'–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–ø {top_n} –º–æ–¥–µ–ª–µ–π')
            plt.grid(True, alpha=0.3)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width, bar.get_y() + bar.get_height()/2,
                        f'{width:.4f}', ha='left', va='center')
            
            # Box plot –¥–ª—è —Ç–æ–ø-5 –º–æ–¥–µ–ª–µ–π —Å CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            plt.subplot(2, 1, 2)
            top_5_models = [m for m in models[:5] if m in result.cv_results and result.cv_results[m]]
            
            if top_5_models:
                cv_data = [result.cv_results[model] for model in top_5_models]
                plt.boxplot(cv_data, labels=top_5_models)
                plt.xticks(rotation=45, ha='right')
                plt.ylabel('CV –°–∫–æ—Ä')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ CV —Å–∫–æ—Ä–æ–≤ –¥–ª—è —Ç–æ–ø-5 –º–æ–¥–µ–ª–µ–π')
                plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
    
    def get_model_recommendations(
        self,
        result: ModelSelectionResult,
        data_size: int,
        feature_count: int
    ) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–µ–π"""
        
        recommendations = {}
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        if data_size < 1000:
            recommendations['data_size'] = "–ù–µ–±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç: —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ (Linear, Ridge, Lasso)"
        elif data_size < 10000:
            recommendations['data_size'] = "–°—Ä–µ–¥–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç: –ø–æ–¥—Ö–æ–¥—è—Ç Random Forest, Gradient Boosting"
        else:
            recommendations['data_size'] = "–ë–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã XGBoost, LightGBM, CatBoost"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if feature_count < 10:
            recommendations['features'] = "–ú–∞–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ"
        elif feature_count < 100:
            recommendations['features'] = "–£–º–µ—Ä–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –ø–æ–¥—Ö–æ–¥—è—Ç –∞–Ω—Å–∞–º–±–ª–∏"
        else:
            recommendations['features'] = "–ú–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge, Lasso)"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        task_type = result.task_type
        if task_type == 'regression':
            recommendations['task'] = "–†–µ–≥—Ä–µ—Å—Å–∏—è: –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ MSE –∏ R¬≤ –º–µ—Ç—Ä–∏–∫–∏"
        else:
            recommendations['task'] = "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –≤–∞–∂–Ω—ã precision, recall –∏ F1-score"
        
        # –¢–æ–ø –º–æ–¥–µ–ª—å
        if result.best_models:
            best_model = result.best_models[0]
            best_score = result.model_scores[best_model]
            recommendations['best_model'] = f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model} (—Å–∫–æ—Ä: {best_score:.4f})"
        
        return recommendations
    
    def get_selection_report(self, result: ModelSelectionResult) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –æ—Ç–±–æ—Ä—É –º–æ–¥–µ–ª–µ–π"""
        
        report = f"""
=== –û–¢–ß–ï–¢ –ü–û –û–¢–ë–û–†–£ –ú–û–î–ï–õ–ï–ô ===

–¢–∏–ø –∑–∞–¥–∞—á–∏: {result.task_type}
–í—Ä–µ–º—è –æ—Ç–±–æ—Ä–∞: {result.selection_time:.2f}—Å
–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(result.model_scores)}
–£—Å–ø–µ—à–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {result.evaluation_metadata.get('successful_models', 0)}

–¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π:
"""
        
        sorted_models = sorted(
            result.model_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        for i, (model_name, score) in enumerate(sorted_models[:5], 1):
            if model_name in result.cv_results and result.cv_results[model_name]:
                std_score = np.std(result.cv_results[model_name])
                report += f"{i}. {model_name}: {score:.4f} ¬± {std_score:.4f}\n"
            else:
                report += f"{i}. {model_name}: {score:.4f}\n"
        
        report += f"\n–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {result.evaluation_metadata}"
        
        return report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    from ..utils.config_manager import AutoMLConfig
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    # –†–µ–≥—Ä–µ—Å—Å–∏—è
    y_reg = pd.Series(
        X.iloc[:, :5].sum(axis=1) + 0.1 * np.random.randn(n_samples)
    )
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    y_clf = pd.Series((y_reg > y_reg.median()).astype(int))
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
    config = AutoMLConfig()
    selector = ModelSelector(config)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    print("=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–ì–†–ï–°–°–ò–ò ===")
    result_reg = selector.select_best_models(
        X, y_reg,
        models=['linear_regression', 'ridge', 'random_forest', 'xgboost'],
        cv_folds=3,
        top_k=3
    )
    
    print(selector.get_selection_report(result_reg))
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print("\n=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ===")
    result_clf = selector.select_best_models(
        X, y_clf,
        models=['logistic_regression', 'random_forest', 'xgboost'],
        cv_folds=3,
        time_series_split=False
    )
    
    print(selector.get_selection_report(result_clf))
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n=== –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ===")
    recommendations = selector.get_model_recommendations(
        result_reg, data_size=len(X), feature_count=len(X.columns)
    )
    
    for key, rec in recommendations.items():
        print(f"{key}: {rec}")