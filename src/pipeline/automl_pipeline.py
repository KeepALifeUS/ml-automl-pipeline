"""
Main AutoML Pipeline for Crypto Trading Bot v5.0
Orchestrates the complete machine learning workflow with enterprise patterns
"""

import logging
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.preprocessing import StandardScaler, RobustScaler
import joblib
import pickle
import json
import time
from pathlib import Path
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
import matplotlib.pyplot as plt
import seaborn as sns

from ..feature_engineering.auto_feature_generator import AutoFeatureGenerator, FeatureGenerationResult
from ..feature_engineering.feature_selector import AdvancedFeatureSelector, FeatureSelectionResult
from ..optimization.bayesian_optimizer import CryptoMLHyperparameterOptimizer, OptimizationResult
from ..model_selection.model_selector import ModelSelector, ModelSelectionResult
from ..model_selection.ensemble_builder import EnsembleBuilder, EnsembleResult
from ..evaluation.model_evaluator import ModelEvaluator, EvaluationResult
from ..utils.config_manager import AutoMLConfig
from ..utils.data_preprocessor import DataPreprocessor


class PipelineStage(Enum):
    """Stages AutoML pipeline"""
    DATA_PREPROCESSING = "data_preprocessing"
    FEATURE_GENERATION = "feature_generation"
    FEATURE_SELECTION = "feature_selection"
    MODEL_SELECTION = "model_selection"
    HYPERPARAMETER_OPTIMIZATION = "hyperparameter_optimization"
    ENSEMBLE_BUILDING = "ensemble_building"
    MODEL_EVALUATION = "model_evaluation"
    MODEL_DEPLOYMENT = "model_deployment"


@dataclass
class PipelineResult:
    """Result execution AutoML pipeline"""
    best_model: Any
    best_model_name: str
    best_score: float
    feature_generation_result: Optional[FeatureGenerationResult]
    feature_selection_result: Optional[FeatureSelectionResult]
    optimization_results: Dict[str, OptimizationResult]
    ensemble_result: Optional[EnsembleResult]
    evaluation_result: EvaluationResult
    pipeline_metadata: Dict[str, Any]
    total_time: float
    stages_completed: List[str]


class AutoMLPipeline:
    """
    Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ AutoML pipeline for crypto trading
    Implements enterprise patterns for scalable ML systems
    """
    
    def __init__(self, config: Optional[AutoMLConfig] = None, output_dir: Optional[str] = None):
        self.config = config or AutoMLConfig()
        self.output_dir = Path(output_dir or "automl_output")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.preprocessor = DataPreprocessor(self.config)
        self.feature_generator = AutoFeatureGenerator(self.config)
        self.feature_selector = AdvancedFeatureSelector(self.config)
        self.hyperparameter_optimizer = CryptoMLHyperparameterOptimizer(self.config)
        self.model_selector = ModelSelector(self.config)
        self.ensemble_builder = EnsembleBuilder(self.config)
        self.evaluator = ModelEvaluator(self.config)
        
        # Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ pipeline
        self.pipeline_state = {}
        self.console = Console()
        
        logger.info("ğŸš€ AutoML Pipeline initialized")
    
    def run(
        self,
        data: pd.DataFrame,
        target_column: str,
        test_size: float = 0.2,
        validation_size: float = 0.2,
        time_series_split: bool = True,
        stages: Optional[List[str]] = None
    ) -> PipelineResult:
        """
        Launch Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ AutoML pipeline
        
        Args:
            data: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ data
            target_column: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ columns with target variable
            test_size: Size test set
            validation_size: Size Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ set
            time_series_split: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ temporal Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ
            stages: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº stages for execution (by default all)
        """
        start_time = time.time()
        
        self.console.print(
            Panel.fit(
                "ğŸ¤– [bold blue]CRYPTO TRADING AUTOML PIPELINE v5.0[/bold blue] ğŸš€\n"
                f"ğŸ“Š Data: {len(data)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, {len(data.columns)} features\n"
                f"ğŸ¯ Target variable: {target_column}",
                title="Launch AutoML Pipeline"
            )
        )
        
        if stages is None:
            stages = [stage.value for stage in PipelineStage]
        
        stages_completed = []
        pipeline_metadata = {
            'start_time': start_time,
            'data_shape': data.shape,
            'target_column': target_column,
            'config': self.config.dict() if hasattr(self.config, 'dict') else str(self.config)
        }
        
        try:
            # === Stage 1: Preprocessing Data ===
            if PipelineStage.DATA_PREPROCESSING.value in stages:
                logger.info("ğŸ”§ Stage 1: Preprocessing data")
                
                X, y = self._preprocess_data(data, target_column)
                X_train, X_test, y_train, y_test = self._split_data(
                    X, y, test_size, validation_size, time_series_split
                )
                
                stages_completed.append(PipelineStage.DATA_PREPROCESSING.value)
                
                self.console.print("âœ… [green]Preprocessing data Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°[/green]")
            else:
                logger.info("â­ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ data")
                X = data.drop(columns=[target_column])
                y = data[target_column]
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=test_size, random_state=42
                )
            
            # === Stage 2: Generation Features ===
            feature_generation_result = None
            if PipelineStage.FEATURE_GENERATION.value in stages:
                logger.info("ğŸ¨ Stage 2: Generation features")
                
                feature_generation_result = self._generate_features(X_train)
                
                if feature_generation_result and not feature_generation_result.features.empty:
                    # Apply ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… features to training set
                    X_train_enhanced = pd.concat([X_train, feature_generation_result.features], axis=1)
                    
                    # Apply to test set
                    test_features = self.feature_generator.generate_features(X_test)
                    X_test_enhanced = pd.concat([X_test, test_features.features], axis=1)
                    
                    X_train = X_train_enhanced
                    X_test = X_test_enhanced
                
                stages_completed.append(PipelineStage.FEATURE_GENERATION.value)
                self.console.print("âœ… [green]Generation features Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°[/green]")
            
            # === Stage 3: Selection Features ===
            feature_selection_result = None
            if PipelineStage.FEATURE_SELECTION.value in stages:
                logger.info("ğŸ¯ Stage 3: Select features")
                
                feature_selection_result = self._select_features(X_train, y_train)
                
                if feature_selection_result and feature_selection_result.selected_features:
                    X_train = X_train[feature_selection_result.selected_features]
                    X_test = X_test[feature_selection_result.selected_features]
                
                stages_completed.append(PipelineStage.FEATURE_SELECTION.value)
                self.console.print("âœ… [green]Select features Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½[/green]")
            
            # === Stage 4: Selection Models ===
            model_selection_result = None
            if PipelineStage.MODEL_SELECTION.value in stages:
                logger.info("ğŸ¤– Stage 4: Select models")
                
                model_selection_result = self._select_models(X_train, y_train)
                
                stages_completed.append(PipelineStage.MODEL_SELECTION.value)
                self.console.print("âœ… [green]Select models Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½[/green]")
            
            # === Stage 5: Optimization Hyperparameters ===
            optimization_results = {}
            if PipelineStage.HYPERPARAMETER_OPTIMIZATION.value in stages:
                logger.info("âš™ï¸ Stage 5: Optimization hyperparameters")
                
                # Determine models for optimization
                models_to_optimize = []
                if model_selection_result:
                    # Take Ñ‚Ğ¾Ğ¿-3 model
                    sorted_models = sorted(
                        model_selection_result.model_scores.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )
                    models_to_optimize = [model[0] for model in sorted_models[:3]]
                else:
                    # By default optimize base model
                    models_to_optimize = ['xgboost', 'random_forest', 'lightgbm']
                
                optimization_results = self._optimize_hyperparameters(
                    X_train, y_train, models_to_optimize
                )
                
                stages_completed.append(PipelineStage.HYPERPARAMETER_OPTIMIZATION.value)
                self.console.print("âœ… [green]Optimization hyperparameters Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°[/green]")
            
            # === Stage 6: Construction ĞĞĞ¡ĞĞœĞ‘Ğ›Ğ¯ ===
            ensemble_result = None
            if PipelineStage.ENSEMBLE_BUILDING.value in stages:
                logger.info("ğŸ¤ Stage 6: Build Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ")
                
                ensemble_result = self._build_ensemble(
                    X_train, y_train, optimization_results
                )
                
                stages_completed.append(PipelineStage.ENSEMBLE_BUILDING.value)
                self.console.print("âœ… [green]Build Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾[/green]")
            
            # === Stage 7: Evaluation Models ===
            evaluation_result = None
            if PipelineStage.MODEL_EVALUATION.value in stages:
                logger.info("ğŸ“Š Stage 7: Evaluate models")
                
                # Determine best model
                best_model, best_model_name, best_score = self._select_best_model(
                    optimization_results, ensemble_result
                )
                
                evaluation_result = self._evaluate_models(
                    X_train, y_train, X_test, y_test,
                    best_model, best_model_name
                )
                
                stages_completed.append(PipelineStage.MODEL_EVALUATION.value)
                self.console.print("âœ… [green]Evaluate models Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°[/green]")
            
            # === Creation Ğ˜Ğ¢ĞĞ“ĞĞ’ĞĞ“Ğ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ ===
            total_time = time.time() - start_time
            pipeline_metadata.update({
                'end_time': time.time(),
                'stages_completed': stages_completed,
                'final_features_count': X_train.shape[1] if 'X_train' in locals() else 0
            })
            
            result = PipelineResult(
                best_model=best_model if 'best_model' in locals() else None,
                best_model_name=best_model_name if 'best_model_name' in locals() else "unknown",
                best_score=best_score if 'best_score' in locals() else 0.0,
                feature_generation_result=feature_generation_result,
                feature_selection_result=feature_selection_result,
                optimization_results=optimization_results,
                ensemble_result=ensemble_result,
                evaluation_result=evaluation_result,
                pipeline_metadata=pipeline_metadata,
                total_time=total_time,
                stages_completed=stages_completed
            )
            
            # Save results
            self._save_pipeline_results(result)
            
            # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚
            self._print_final_report(result)
            
            logger.info(f"ğŸ‰ AutoML Pipeline Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½ for {total_time:.2f}with")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error in AutoML Pipeline: {e}")
            raise
    
    def _preprocess_data(self, data: pd.DataFrame, target_column: str) -> Tuple[pd.DataFrame, pd.Series]:
        """Preprocessing data"""
        logger.info("ğŸ”§ Preprocessing data...")
        
        # Split on features and target variable
        X = data.drop(columns=[target_column])
        y = data[target_column]
        
        # Preprocessing with Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DataPreprocessor
        X_processed = self.preprocessor.preprocess(X)
        y_processed = self.preprocessor.preprocess_target(y)
        
        logger.info(f"âœ… Data Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹: {X_processed.shape[0]} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹, {X_processed.shape[1]} features")
        
        return X_processed, y_processed
    
    def _split_data(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        test_size: float,
        validation_size: float,
        time_series_split: bool
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """Split data on training and test set"""
        logger.info("âœ‚ï¸ Split data...")
        
        if time_series_split:
            # Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ split (without Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ)
            split_idx = int(len(X) * (1 - test_size))
            X_train = X.iloc[:split_idx]
            X_test = X.iloc[split_idx:]
            y_train = y.iloc[:split_idx]
            y_test = y.iloc[split_idx:]
        else:
            # Random split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42
            )
        
        logger.info(f"âœ… Data Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ñ‹: training={len(X_train)}, test={len(X_test)}")
        
        return X_train, X_test, y_train, y_test
    
    def _generate_features(self, X: pd.DataFrame) -> Optional[FeatureGenerationResult]:
        """Generation features"""
        logger.info("ğŸ¨ Generation features...")
        
        try:
            result = self.feature_generator.generate_features(X, parallel=True)
            
            logger.info(f"âœ… Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ {len(result.feature_names)} features")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ features: {e}")
            return None
    
    def _select_features(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Optional[FeatureSelectionResult]:
        """Select features"""
        logger.info("ğŸ¯ Select features...")
        
        try:
            result = self.feature_selector.select_features(
                X, y,
                ensemble_selection=True
            )
            
            logger.info(f"âœ… ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾ {len(result.selected_features)} features")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° features: {e}")
            return None
    
    def _select_models(self, X: pd.DataFrame, y: pd.Series) -> Optional[ModelSelectionResult]:
        """Select models"""
        logger.info("ğŸ¤– Select models...")
        
        try:
            result = self.model_selector.select_best_models(
                X, y,
                models=['xgboost', 'random_forest', 'lightgbm', 'ridge', 'elasticnet'],
                cv_folds=3  # ĞœĞµĞ½ÑŒÑˆĞµ folds for speed
            )
            
            logger.info(f"âœ… Tested {len(result.model_scores)} models")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° models: {e}")
            return None
    
    def _optimize_hyperparameters(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        models: List[str]
    ) -> Dict[str, OptimizationResult]:
        """Optimization hyperparameters"""
        logger.info(f"âš™ï¸ Optimization {len(models)} models...")
        
        try:
            results = self.hyperparameter_optimizer.optimize_multiple_models(
                X, y,
                models=models,
                optimizer_method='optuna_tpe',
                n_calls=50  # ĞœĞµĞ½ÑŒÑˆĞµ iterations for speed
            )
            
            logger.info(f"âœ… ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ hyperparameters for {len(results)} models")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Error optimization: {e}")
            return {}
    
    def _build_ensemble(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        optimization_results: Dict[str, OptimizationResult]
    ) -> Optional[EnsembleResult]:
        """Build Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ"""
        logger.info("ğŸ¤ Build Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ...")
        
        try:
            # Create models with optimal parameters
            models = {}
            for model_name, result in optimization_results.items():
                model = self.hyperparameter_optimizer._get_model(model_name, result.best_params)
                models[model_name] = model
            
            if not models:
                logger.warning("âš ï¸ No models for Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ")
                return None
            
            result = self.ensemble_builder.build_ensemble(
                X, y,
                models,
                ensemble_methods=['voting', 'stacking']
            )
            
            logger.info(f"âœ… Ensemble Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ with {len(models)} Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ: {e}")
            return None
    
    def _select_best_model(
        self,
        optimization_results: Dict[str, OptimizationResult],
        ensemble_result: Optional[EnsembleResult]
    ) -> Tuple[Any, str, float]:
        """Select best model"""
        logger.info("ğŸ† Select best model...")
        
        best_model = None
        best_model_name = "unknown"
        best_score = float('inf')
        
        # Check Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… models
        for model_name, result in optimization_results.items():
            if result.best_score < best_score:
                best_score = result.best_score
                best_model_name = model_name
                best_model = self.hyperparameter_optimizer._get_model(
                    model_name, result.best_params
                )
        
        # Check Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ
        if ensemble_result and ensemble_result.best_ensemble_score < best_score:
            best_score = ensemble_result.best_ensemble_score
            best_model_name = f"ensemble_{ensemble_result.best_ensemble_method}"
            best_model = ensemble_result.ensembles[ensemble_result.best_ensemble_method]
        
        logger.info(f"âœ… Best model: {best_model_name} (score: {abs(best_score):.4f})")
        
        return best_model, best_model_name, best_score
    
    def _evaluate_models(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        best_model: Any,
        best_model_name: str
    ) -> EvaluationResult:
        """Evaluate models"""
        logger.info("ğŸ“Š Evaluate best model...")
        
        try:
            # Training best model
            best_model.fit(X_train, y_train)
            
            # Predictions
            y_pred_train = best_model.predict(X_train)
            y_pred_test = best_model.predict(X_test)
            
            result = self.evaluator.evaluate_model(
                best_model,
                X_train, y_train,
                X_test, y_test,
                model_name=best_model_name
            )
            
            logger.info(f"âœ… Model Ğ¾Ñ†ĞµĞ½ĞµĞ½Ğ°: RÂ² = {result.test_r2:.4f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error Ğ¾Ñ†ĞµĞ½ĞºĞ¸ model: {e}")
            # Return empty result
            return EvaluationResult(
                model_name=best_model_name,
                train_mse=0.0, train_mae=0.0, train_r2=0.0,
                test_mse=0.0, test_mae=0.0, test_r2=0.0,
                cross_val_scores=[], feature_importance={},
                evaluation_metadata={}, evaluation_time=0.0
            )
    
    def _save_pipeline_results(self, result: PipelineResult):
        """Save results pipeline"""
        logger.info("ğŸ’¾ Save results...")
        
        try:
            # Save best model
            if result.best_model:
                model_path = self.output_dir / "best_model.pkl"
                joblib.dump(result.best_model, model_path)
                logger.info(f"ğŸ’¾ Model ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {model_path}")
            
            # Save Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            metadata_path = self.output_dir / "pipeline_metadata.json"
            with open(metadata_path, 'w') as f:
                # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ results in serializable Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
                serializable_metadata = {
                    'best_model_name': result.best_model_name,
                    'best_score': result.best_score,
                    'total_time': result.total_time,
                    'stages_completed': result.stages_completed,
                    'pipeline_metadata': result.pipeline_metadata
                }
                json.dump(serializable_metadata, f, indent=2)
            
            logger.info(f"âœ… ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ results: {e}")
    
    def _print_final_report(self, result: PipelineResult):
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°"""
        
        # Create Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ with Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸
        table = Table(title="ğŸ¯ Results AUTOML PIPELINE")
        
        table.add_column("ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ°", style="cyan", no_wrap=True)
        table.add_column("Value", style="magenta")
        
        table.add_row("ğŸ† Best model", result.best_model_name)
        table.add_row("ğŸ“Š Best score", f"{abs(result.best_score):.4f}")
        table.add_row("â±ï¸ Ğ’Ñ€ĞµĞ¼Ñ execution", f"{result.total_time:.2f}with")
        table.add_row("ğŸ¯ Stages Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾", f"{len(result.stages_completed)}/8")
        
        if result.feature_generation_result:
            table.add_row("ğŸ¨ Features ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾", str(len(result.feature_generation_result.feature_names)))
        
        if result.feature_selection_result:
            table.add_row("ğŸ” Features Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ¾", str(len(result.feature_selection_result.selected_features)))
        
        if result.optimization_results:
            table.add_row("âš™ï¸ Models Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾", str(len(result.optimization_results)))
        
        if result.evaluation_result:
            table.add_row("ğŸ“ˆ RÂ² on Ñ‚ĞµÑÑ‚Ğµ", f"{result.evaluation_result.test_r2:.4f}")
            table.add_row("ğŸ“‰ MSE on Ñ‚ĞµÑÑ‚Ğµ", f"{result.evaluation_result.test_mse:.4f}")
        
        self.console.print(table)
        
        # Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸ stages
        stages_panel = Panel(
            " â†’ ".join(result.stages_completed),
            title="ğŸ”„ Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ stages"
        )
        self.console.print(stages_panel)


if __name__ == "__main__":
    # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ use AutoML Pipeline
    
    # Create test data (Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ cryptocurrency data)
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', periods=2000, freq='1H')
    
    # Base OHLCV data
    data = pd.DataFrame({
        'open': np.random.randn(2000).cumsum() + 50000,
        'high': np.random.randn(2000).cumsum() + 50100,
        'low': np.random.randn(2000).cumsum() + 49900,
        'close': np.random.randn(2000).cumsum() + 50000,
        'volume': np.random.exponential(1000, 2000)
    }, index=dates)
    
    # Target variable (Ğ±ÑƒĞ´ÑƒÑ‰Ğ°Ñ return)
    data['future_return'] = data['close'].shift(-1) / data['close'] - 1
    data = data.dropna()
    
    # Create and launch pipeline
    config = AutoMLConfig()
    pipeline = AutoMLPipeline(config, output_dir="test_automl_output")
    
    result = pipeline.run(
        data=data,
        target_column='future_return',
        test_size=0.2,
        time_series_split=True
    )
    
    print(f"\nğŸ‰ AutoML Pipeline Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½!")
    print(f"ğŸ† Best model: {result.best_model_name}")
    print(f"ğŸ“Š Best score: {abs(result.best_score):.4f}")
    print(f"â±ï¸ Ğ’Ñ€ĞµĞ¼Ñ execution: {result.total_time:.2f}with")