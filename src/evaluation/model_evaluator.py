"""
Comprehensive Model Evaluator for Crypto Trading AutoML
Implements Context7 enterprise patterns for thorough model evaluation
"""

import logging
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.model_selection import cross_val_score, cross_validate, TimeSeriesSplit
from sklearn.inspection import permutation_importance
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
import time
import joblib
from pathlib import Path

from ..utils.config_manager import AutoMLConfig


@dataclass
class EvaluationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏"""
    model_name: str
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    train_mse: float
    train_mae: float
    train_r2: float
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    test_mse: float
    test_mae: float
    test_r2: float
    
    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    cross_val_scores: List[float]
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance: Dict[str, float]
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    evaluation_metadata: Dict[str, Any]
    evaluation_time: float


class CryptoTradingMetrics:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞"""
    
    @staticmethod
    def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –®–∞—Ä–ø–∞"""
        try:
            if len(returns) == 0:
                return 0.0
            
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            if std_return == 0:
                return 0.0
            
            return (mean_return - risk_free_rate) / std_return
        except Exception:
            return 0.0
    
    @staticmethod
    def sortino_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –°–æ—Ä—Ç–∏–Ω–æ"""
        try:
            if len(returns) == 0:
                return 0.0
            
            mean_return = np.mean(returns)
            downside_returns = returns[returns < risk_free_rate]
            
            if len(downside_returns) == 0:
                return float('inf') if mean_return > risk_free_rate else 0.0
            
            downside_deviation = np.std(downside_returns)
            
            if downside_deviation == 0:
                return 0.0
            
            return (mean_return - risk_free_rate) / downside_deviation
        except Exception:
            return 0.0
    
    @staticmethod
    def maximum_drawdown(returns: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ—Å–∞–¥–∫–∏"""
        try:
            if len(returns) == 0:
                return 0.0
            
            cumulative_returns = np.cumprod(1 + returns)
            peak = np.maximum.accumulate(cumulative_returns)
            drawdown = (cumulative_returns - peak) / peak
            
            return np.min(drawdown)
        except Exception:
            return 0.0
    
    @staticmethod
    def win_rate(predictions: np.ndarray, actuals: np.ndarray, threshold: float = 0.0) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫"""
        try:
            if len(predictions) == 0 or len(actuals) == 0:
                return 0.0
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            pred_direction = predictions > threshold
            actual_direction = actuals > threshold
            
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            correct_predictions = pred_direction == actual_direction
            
            return np.mean(correct_predictions)
        except Exception:
            return 0.0
    
    @staticmethod
    def profit_factor(predictions: np.ndarray, actuals: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∏–±—ã–ª–∏"""
        try:
            if len(predictions) == 0 or len(actuals) == 0:
                return 0.0
            
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—Ö–æ–¥–∏–º –≤ –ø–æ–∑–∏—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            # –∏ –ø–æ–ª—É—á–∞–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Ä–∞–≤–Ω—É—é –∞–∫—Ç—É–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é
            profitable_trades = actuals[predictions > 0]
            losing_trades = actuals[predictions < 0]
            
            gross_profit = np.sum(profitable_trades[profitable_trades > 0])
            gross_loss = -np.sum(losing_trades[losing_trades < 0])
            
            if gross_loss == 0:
                return float('inf') if gross_profit > 0 else 0.0
            
            return gross_profit / gross_loss
        except Exception:
            return 0.0
    
    @staticmethod
    def information_ratio(predictions: np.ndarray, actuals: np.ndarray, benchmark_return: float = 0.0) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞"""
        try:
            if len(predictions) == 0 or len(actuals) == 0:
                return 0.0
            
            # –ê–∫—Ç–∏–≤–Ω—ã–π –¥–æ—Ö–æ–¥ (–ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥ –±–µ–Ω—á–º–∞—Ä–∫–æ–º)
            active_returns = actuals - benchmark_return
            
            # –û—à–∏–±–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
            tracking_error = np.std(active_returns)
            
            if tracking_error == 0:
                return 0.0
            
            return np.mean(active_returns) / tracking_error
        except Exception:
            return 0.0


class ModelEvaluator:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    –†–µ–∞–ª–∏–∑—É–µ—Ç Context7 enterprise patterns
    """
    
    def __init__(self, config: Optional[AutoMLConfig] = None):
        self.config = config or AutoMLConfig()
        self.evaluation_config = self.config.model_evaluation
        self.console = Console()
        
        # –ö—ç—à –¥–ª—è SHAP –æ–±—ä–µ–∫—Ç–æ–≤ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        self.shap_cache = {}
        
        logger.info("üìä ModelEvaluator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def evaluate_model(
        self,
        model: Any,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        model_name: str = "unknown",
        task_type: str = "regression"
    ) -> EvaluationResult:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
            X_train: –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
            y_train: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
            X_test: –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            y_test: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (regression/classification)
        """
        start_time = time.time()
        
        logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        
        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
            ) as progress:
                
                task = progress.add_task("–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...", total=None)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                progress.update(task, description="–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
                y_train_pred = model.predict(X_train)
                y_test_pred = model.predict(X_test)
                
                # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                progress.update(task, description="–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")
                if task_type == "regression":
                    metrics = self._calculate_regression_metrics(
                        y_train, y_train_pred, y_test, y_test_pred
                    )
                else:
                    metrics = self._calculate_classification_metrics(
                        y_train, y_train_pred, y_test, y_test_pred
                    )
                
                # –ö—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                progress.update(task, description="–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫...")
                crypto_metrics = self._calculate_crypto_metrics(
                    y_test_pred, y_test
                )
                
                # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                progress.update(task, description="–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è...")
                cv_scores = self._perform_cross_validation(
                    model, X_train, y_train, task_type
                )
                
                # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                progress.update(task, description="–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
                feature_importance = self._calculate_feature_importance(
                    model, X_train, y_train, model_name
                )
                
                progress.update(task, description="‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", completed=True)
            
            evaluation_time = time.time() - start_time
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if task_type == "regression":
                result = EvaluationResult(
                    model_name=model_name,
                    train_mse=metrics['train_mse'],
                    train_mae=metrics['train_mae'],
                    train_r2=metrics['train_r2'],
                    test_mse=metrics['test_mse'],
                    test_mae=metrics['test_mae'],
                    test_r2=metrics['test_r2'],
                    cross_val_scores=cv_scores,
                    feature_importance=feature_importance,
                    evaluation_metadata={
                        'task_type': task_type,
                        'crypto_metrics': crypto_metrics,
                        'train_samples': len(y_train),
                        'test_samples': len(y_test),
                        'features_count': len(X_train.columns)
                    },
                    evaluation_time=evaluation_time
                )
            else:
                # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                result = EvaluationResult(
                    model_name=model_name,
                    train_mse=0.0,  # –ù–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ
                    train_mae=0.0,  # –ù–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ
                    train_r2=metrics.get('train_accuracy', 0.0),
                    test_mse=0.0,   # –ù–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ
                    test_mae=0.0,   # –ù–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ
                    test_r2=metrics.get('test_accuracy', 0.0),
                    cross_val_scores=cv_scores,
                    feature_importance=feature_importance,
                    evaluation_metadata={
                        'task_type': task_type,
                        'classification_metrics': metrics,
                        'train_samples': len(y_train),
                        'test_samples': len(y_test),
                        'features_count': len(X_train.columns)
                    },
                    evaluation_time=evaluation_time
                )
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self._print_evaluation_results(result)
            
            logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {evaluation_time:.2f}—Å")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            evaluation_time = time.time() - start_time
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return EvaluationResult(
                model_name=model_name,
                train_mse=0.0, train_mae=0.0, train_r2=0.0,
                test_mse=0.0, test_mae=0.0, test_r2=0.0,
                cross_val_scores=[], feature_importance={},
                evaluation_metadata={'error': str(e)},
                evaluation_time=evaluation_time
            )
    
    def _calculate_regression_metrics(
        self,
        y_train: pd.Series,
        y_train_pred: np.ndarray,
        y_test: pd.Series,
        y_test_pred: np.ndarray
    ) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        
        metrics = {}
        
        # –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞
        metrics['train_mse'] = mean_squared_error(y_train, y_train_pred)
        metrics['train_mae'] = mean_absolute_error(y_train, y_train_pred)
        metrics['train_r2'] = r2_score(y_train, y_train_pred)
        metrics['train_rmse'] = np.sqrt(metrics['train_mse'])
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞
        metrics['test_mse'] = mean_squared_error(y_test, y_test_pred)
        metrics['test_mae'] = mean_absolute_error(y_test, y_test_pred)
        metrics['test_r2'] = r2_score(y_test, y_test_pred)
        metrics['test_rmse'] = np.sqrt(metrics['test_mse'])
        
        # MAPE (Mean Absolute Percentage Error)
        try:
            def mape(actual, pred):
                mask = actual != 0
                if mask.sum() == 0:
                    return 0.0
                return np.mean(np.abs((actual[mask] - pred[mask]) / actual[mask])) * 100
            
            metrics['train_mape'] = mape(y_train.values, y_train_pred)
            metrics['test_mape'] = mape(y_test.values, y_test_pred)
        except:
            metrics['train_mape'] = 0.0
            metrics['test_mape'] = 0.0
        
        return metrics
    
    def _calculate_classification_metrics(
        self,
        y_train: pd.Series,
        y_train_pred: np.ndarray,
        y_test: pd.Series,
        y_test_pred: np.ndarray
    ) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        
        metrics = {}
        
        # –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞
        metrics['train_accuracy'] = accuracy_score(y_train, y_train_pred)
        metrics['train_precision'] = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)
        metrics['train_recall'] = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)
        metrics['train_f1'] = f1_score(y_train, y_train_pred, average='weighted', zero_division=0)
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞
        metrics['test_accuracy'] = accuracy_score(y_test, y_test_pred)
        metrics['test_precision'] = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)
        metrics['test_recall'] = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)
        metrics['test_f1'] = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)
        
        # AUC (–µ—Å–ª–∏ –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
        try:
            if len(np.unique(y_test)) == 2:
                # –ù—É–∂–Ω—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è AUC
                if hasattr(self, 'predict_proba'):
                    y_test_proba = y_test_pred  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ —ç—Ç–æ —É–∂–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
                    metrics['test_auc'] = roc_auc_score(y_test, y_test_proba)
                else:
                    metrics['test_auc'] = 0.0
            else:
                metrics['test_auc'] = 0.0
        except:
            metrics['test_auc'] = 0.0
        
        return metrics
    
    def _calculate_crypto_metrics(
        self,
        predictions: np.ndarray,
        actuals: pd.Series
    ) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞"""
        
        crypto_metrics = {}
        
        try:
            actuals_array = actuals.values
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –®–∞—Ä–ø–∞
            crypto_metrics['sharpe_ratio'] = CryptoTradingMetrics.sharpe_ratio(actuals_array)
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –°–æ—Ä—Ç–∏–Ω–æ
            crypto_metrics['sortino_ratio'] = CryptoTradingMetrics.sortino_ratio(actuals_array)
            
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞
            crypto_metrics['max_drawdown'] = CryptoTradingMetrics.maximum_drawdown(actuals_array)
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–æ–∫
            crypto_metrics['win_rate'] = CryptoTradingMetrics.win_rate(predictions, actuals_array)
            
            # –§–∞–∫—Ç–æ—Ä –ø—Ä–∏–±—ã–ª–∏
            crypto_metrics['profit_factor'] = CryptoTradingMetrics.profit_factor(predictions, actuals_array)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
            crypto_metrics['information_ratio'] = CryptoTradingMetrics.information_ratio(
                predictions, actuals_array
            )
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            crypto_metrics['prediction_correlation'] = np.corrcoef(predictions, actuals_array)[0, 1]
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫—Ä–∏–ø—Ç–æ–º–µ—Ç—Ä–∏–∫: {e}")
            crypto_metrics = {
                'sharpe_ratio': 0.0,
                'sortino_ratio': 0.0,
                'max_drawdown': 0.0,
                'win_rate': 0.0,
                'profit_factor': 0.0,
                'information_ratio': 0.0,
                'prediction_correlation': 0.0
            }
        
        return crypto_metrics
    
    def _perform_cross_validation(
        self,
        model: Any,
        X: pd.DataFrame,
        y: pd.Series,
        task_type: str
    ) -> List[float]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        
        try:
            cv_folds = self.evaluation_config.cv_folds
            cv_scoring = self.evaluation_config.cv_scoring
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if cv_scoring is None:
                if task_type == "regression":
                    cv_scoring = 'r2'
                else:
                    cv_scoring = 'accuracy'
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ TimeSeriesSplit –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            if hasattr(self.config, 'crypto_specific') and self.config.crypto_specific.get('walk_forward_validation', False):
                cv = TimeSeriesSplit(n_splits=cv_folds)
            else:
                cv = cv_folds
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
            scores = cross_val_score(
                model, X, y,
                cv=cv,
                scoring=cv_scoring,
                n_jobs=-1,
                error_score='raise'
            )
            
            return scores.tolist()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return []
    
    def _calculate_feature_importance(
        self,
        model: Any,
        X: pd.DataFrame,
        y: pd.Series,
        model_name: str
    ) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        
        try:
            method = self.evaluation_config.feature_importance_method
            
            if method == 'built_in' and hasattr(model, 'feature_importances_'):
                # –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                importances = model.feature_importances_
                return dict(zip(X.columns, importances))
                
            elif method == 'permutation':
                # Permutation importance
                try:
                    perm_importance = permutation_importance(
                        model, X, y,
                        n_repeats=5,
                        random_state=42,
                        n_jobs=-1
                    )
                    return dict(zip(X.columns, perm_importance.importances_mean))
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ permutation importance: {e}")
                    return {}
                    
            elif method == 'shap' and model_name not in self.shap_cache:
                # SHAP values (–±–æ–ª–µ–µ –º–µ–¥–ª–µ–Ω–Ω—ã–π, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π)
                try:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è SHAP
                    sample_size = min(100, len(X))
                    X_sample = X.sample(n=sample_size, random_state=42)
                    
                    if hasattr(model, 'predict_proba'):
                        explainer = shap.TreeExplainer(model)
                    else:
                        explainer = shap.Explainer(model, X_sample)
                    
                    shap_values = explainer.shap_values(X_sample)
                    
                    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∫–ª–∞—Å—Å
                    if isinstance(shap_values, list):
                        shap_values = shap_values[0]
                    
                    # –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                    mean_abs_shap = np.mean(np.abs(shap_values), axis=0)
                    
                    self.shap_cache[model_name] = dict(zip(X.columns, mean_abs_shap))
                    return self.shap_cache[model_name]
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ SHAP importance: {e}")
                    return {}
            
            # Fallback: —Ä–∞–≤–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            return {col: 1.0 / len(X.columns) for col in X.columns}
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return {}
    
    def _print_evaluation_results(self, result: EvaluationResult):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏"""
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        table = Table(title=f"üìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò: {result.model_name.upper()}")
        
        table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan", no_wrap=True)
        table.add_column("–û–±—É—á–µ–Ω–∏–µ", style="green")
        table.add_column("–¢–µ—Å—Ç", style="magenta")
        
        if result.evaluation_metadata.get('task_type') == 'regression':
            table.add_row("R¬≤", f"{result.train_r2:.4f}", f"{result.test_r2:.4f}")
            table.add_row("MSE", f"{result.train_mse:.4f}", f"{result.test_mse:.4f}")
            table.add_row("MAE", f"{result.train_mae:.4f}", f"{result.test_mae:.4f}")
            
            if 'train_rmse' in result.evaluation_metadata.get('crypto_metrics', {}):
                table.add_row("RMSE", f"{np.sqrt(result.train_mse):.4f}", f"{np.sqrt(result.test_mse):.4f}")
        else:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            metrics = result.evaluation_metadata.get('classification_metrics', {})
            table.add_row("Accuracy", f"{metrics.get('train_accuracy', 0):.4f}", f"{metrics.get('test_accuracy', 0):.4f}")
            table.add_row("Precision", f"{metrics.get('train_precision', 0):.4f}", f"{metrics.get('test_precision', 0):.4f}")
            table.add_row("Recall", f"{metrics.get('train_recall', 0):.4f}", f"{metrics.get('test_recall', 0):.4f}")
            table.add_row("F1-Score", f"{metrics.get('train_f1', 0):.4f}", f"{metrics.get('test_f1', 0):.4f}")
        
        self.console.print(table)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        if result.cross_val_scores:
            cv_mean = np.mean(result.cross_val_scores)
            cv_std = np.std(result.cross_val_scores)
            cv_info = f"üìä –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {cv_mean:.4f} ¬± {cv_std:.4f} (n={len(result.cross_val_scores)})"
            self.console.print(cv_info)
        
        # –ö—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        crypto_metrics = result.evaluation_metadata.get('crypto_metrics', {})
        if crypto_metrics:
            crypto_table = Table(title="üí∞ –ö–†–ò–ü–¢–û–¢–†–ï–ô–î–ò–ù–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò")
            crypto_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
            crypto_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="yellow")
            
            for metric, value in crypto_metrics.items():
                if isinstance(value, float):
                    crypto_table.add_row(metric.replace('_', ' ').title(), f"{value:.4f}")
            
            self.console.print(crypto_table)
        
        # –¢–æ–ø –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if result.feature_importance:
            top_features = sorted(
                result.feature_importance.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:10]
            
            if top_features:
                features_info = "üîç –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n"
                for i, (feature, importance) in enumerate(top_features, 1):
                    features_info += f"  {i:2d}. {feature}: {importance:.4f}\n"
                
                self.console.print(features_info)
    
    def compare_models(
        self,
        results: List[EvaluationResult],
        save_path: Optional[str] = None
    ):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        
        if not results:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return
        
        logger.info(f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ {len(results)} –º–æ–¥–µ–ª–µ–π")
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_table = Table(title="üèÜ –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        
        comparison_table.add_column("–†–∞–Ω–≥", style="cyan", no_wrap=True)
        comparison_table.add_column("–ú–æ–¥–µ–ª—å", style="magenta")
        comparison_table.add_column("Test R¬≤", style="green")
        comparison_table.add_column("Test RMSE", style="yellow")
        comparison_table.add_column("CV Score", style="blue")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ test R¬≤
        sorted_results = sorted(results, key=lambda x: x.test_r2, reverse=True)
        
        for i, result in enumerate(sorted_results, 1):
            cv_mean = np.mean(result.cross_val_scores) if result.cross_val_scores else 0.0
            test_rmse = np.sqrt(result.test_mse) if result.test_mse > 0 else 0.0
            
            comparison_table.add_row(
                str(i),
                result.model_name,
                f"{result.test_r2:.4f}",
                f"{test_rmse:.4f}",
                f"{cv_mean:.4f}"
            )
        
        self.console.print(comparison_table)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        if save_path:
            self.plot_models_comparison(results, save_path)
    
    def plot_models_comparison(
        self,
        results: List[EvaluationResult],
        save_path: str
    ):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            model_names = [r.model_name for r in results]
            
            # –ì—Ä–∞—Ñ–∏–∫ 1: R¬≤ scores
            train_r2 = [r.train_r2 for r in results]
            test_r2 = [r.test_r2 for r in results]
            
            x = np.arange(len(model_names))
            width = 0.35
            
            axes[0, 0].bar(x - width/2, train_r2, width, label='Train', alpha=0.7)
            axes[0, 0].bar(x + width/2, test_r2, width, label='Test', alpha=0.7)
            axes[0, 0].set_xlabel('–ú–æ–¥–µ–ª–∏')
            axes[0, 0].set_ylabel('R¬≤ Score')
            axes[0, 0].set_title('R¬≤ Score –ø–æ –º–æ–¥–µ–ª—è–º')
            axes[0, 0].set_xticks(x)
            axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 2: RMSE
            train_rmse = [np.sqrt(r.train_mse) for r in results]
            test_rmse = [np.sqrt(r.test_mse) for r in results]
            
            axes[0, 1].bar(x - width/2, train_rmse, width, label='Train', alpha=0.7)
            axes[0, 1].bar(x + width/2, test_rmse, width, label='Test', alpha=0.7)
            axes[0, 1].set_xlabel('–ú–æ–¥–µ–ª–∏')
            axes[0, 1].set_ylabel('RMSE')
            axes[0, 1].set_title('RMSE –ø–æ –º–æ–¥–µ–ª—è–º')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 3: Cross-validation scores
            cv_means = [np.mean(r.cross_val_scores) if r.cross_val_scores else 0 for r in results]
            cv_stds = [np.std(r.cross_val_scores) if r.cross_val_scores else 0 for r in results]
            
            axes[1, 0].bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7)
            axes[1, 0].set_xlabel('–ú–æ–¥–µ–ª–∏')
            axes[1, 0].set_ylabel('CV Score')
            axes[1, 0].set_title('Cross-validation Scores')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
            
            # –ì—Ä–∞—Ñ–∏–∫ 4: –í—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏
            eval_times = [r.evaluation_time for r in results]
            
            axes[1, 1].bar(model_names, eval_times, alpha=0.7, color='orange')
            axes[1, 1].set_xlabel('–ú–æ–¥–µ–ª–∏')
            axes[1, 1].set_ylabel('–í—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏ (—Å)')
            axes[1, 1].set_title('–í—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π')
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            
            plt.close()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
    
    def generate_evaluation_report(
        self,
        results: List[EvaluationResult],
        save_path: Optional[str] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –æ—Ü–µ–Ω–∫–µ"""
        
        if not results:
            return "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞"
        
        report = f"""
=== –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–û –û–¶–ï–ù–ö–ï –ú–û–î–ï–õ–ï–ô ===

–î–∞—Ç–∞: {time.strftime('%Y-%m-%d %H:%M:%S')}
–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π: {len(results)}

"""
        
        # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        report += "–°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê:\n"
        report += f"{'–ú–æ–¥–µ–ª—å':<20} {'Test R¬≤':<10} {'Test RMSE':<12} {'CV Mean':<10} {'–í—Ä–µ–º—è':<8}\n"
        report += "-" * 70 + "\n"
        
        sorted_results = sorted(results, key=lambda x: x.test_r2, reverse=True)
        
        for result in sorted_results:
            cv_mean = np.mean(result.cross_val_scores) if result.cross_val_scores else 0.0
            test_rmse = np.sqrt(result.test_mse)
            
            report += f"{result.model_name:<20} {result.test_r2:<10.4f} {test_rmse:<12.4f} {cv_mean:<10.4f} {result.evaluation_time:<8.2f}\n"
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        report += "\n" + "="*70 + "\n"
        report += "–î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ü–û –ú–û–î–ï–õ–Ø–ú:\n\n"
        
        for i, result in enumerate(sorted_results, 1):
            report += f"{i}. –ú–û–î–ï–õ–¨: {result.model_name.upper()}\n"
            report += "-" * 40 + "\n"
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            report += f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞:\n"
            report += f"  R¬≤: {result.train_r2:.4f}\n"
            report += f"  MSE: {result.train_mse:.4f}\n"
            report += f"  MAE: {result.train_mae:.4f}\n"
            
            report += f"\n–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞:\n"
            report += f"  R¬≤: {result.test_r2:.4f}\n"
            report += f"  MSE: {result.test_mse:.4f}\n"
            report += f"  MAE: {result.test_mae:.4f}\n"
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            if result.cross_val_scores:
                cv_mean = np.mean(result.cross_val_scores)
                cv_std = np.std(result.cross_val_scores)
                report += f"\n–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è:\n"
                report += f"  –°—Ä–µ–¥–Ω–µ–µ: {cv_mean:.4f}\n"
                report += f"  –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {cv_std:.4f}\n"
                report += f"  –ú–∏–Ω–∏–º—É–º: {np.min(result.cross_val_scores):.4f}\n"
                report += f"  –ú–∞–∫—Å–∏–º—É–º: {np.max(result.cross_val_scores):.4f}\n"
            
            # –ö—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            crypto_metrics = result.evaluation_metadata.get('crypto_metrics', {})
            if crypto_metrics:
                report += f"\n–ö—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n"
                for metric, value in crypto_metrics.items():
                    if isinstance(value, float):
                        report += f"  {metric.replace('_', ' ').title()}: {value:.4f}\n"
            
            # –¢–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏
            if result.feature_importance:
                top_features = sorted(
                    result.feature_importance.items(),
                    key=lambda x: abs(x[1]),
                    reverse=True
                )[:5]
                
                report += f"\n–¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n"
                for j, (feature, importance) in enumerate(top_features, 1):
                    report += f"  {j}. {feature}: {importance:.4f}\n"
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            report += f"\n–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:\n"
            report += f"  –í—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏: {result.evaluation_time:.2f}—Å\n"
            report += f"  –û–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {result.evaluation_metadata.get('train_samples', 'N/A')}\n"
            report += f"  –¢–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {result.evaluation_metadata.get('test_samples', 'N/A')}\n"
            report += f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {result.evaluation_metadata.get('features_count', 'N/A')}\n"
            
            report += "\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report += "="*70 + "\n"
        report += "–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n\n"
        
        best_model = sorted_results[0]
        report += f"üèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model.model_name}\n"
        report += f"üìä Test R¬≤: {best_model.test_r2:.4f}\n"
        
        if len(results) > 1:
            second_best = sorted_results[1]
            improvement = ((best_model.test_r2 - second_best.test_r2) / second_best.test_r2 * 100) if second_best.test_r2 > 0 else 0
            report += f"üìà –ü—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å—é: {improvement:.2f}%\n"
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        overfitting = best_model.train_r2 - best_model.test_r2
        if overfitting > 0.1:
            report += f"‚ö†Ô∏è  –í–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (—Ä–∞–∑–Ω–æ—Å—Ç—å Train-Test R¬≤: {overfitting:.4f})\n"
            report += "   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Å–∏–ª–µ–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —Å–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"üìù –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        
        return report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ModelEvaluator
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.linear_model import Ridge
    from sklearn.model_selection import train_test_split
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples, n_features = 1000, 20
    
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'feature_{i}' for i in range(n_features)]
    )
    
    y = pd.Series(
        X.iloc[:, :5].sum(axis=1) + 0.1 * np.random.randn(n_samples)
    )
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models = {
        'ridge': Ridge(alpha=1.0),
        'random_forest': RandomForestRegressor(n_estimators=50, random_state=42)
    }
    
    results = []
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–∞
    config = AutoMLConfig()
    evaluator = ModelEvaluator(config)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for name, model in models.items():
        model.fit(X_train, y_train)
        
        result = evaluator.evaluate_model(
            model, X_train, y_train, X_test, y_test,
            model_name=name
        )
        
        results.append(result)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    evaluator.compare_models(results)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = evaluator.generate_evaluation_report(results)
    print("\n" + "="*50)
    print("–ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢:")
    print(report[:1000] + "...")