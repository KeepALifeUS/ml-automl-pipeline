"""
Advanced Data Preprocessor for Crypto Trading AutoML
Implements enterprise patterns for robust data preprocessing
"""

import logging
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    StandardScaler, RobustScaler, MinMaxScaler, QuantileUniformTransformer,
    LabelEncoder, OneHotEncoder, TargetEncoder
)
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import VarianceThreshold
from scipy import stats
from scipy.stats import boxcox, yeojohnson
import pandas_ta as pta
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn
import joblib
from pathlib import Path

from .config_manager import AutoMLConfig, DataPreprocessingConfig


@dataclass
class PreprocessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    processed_data: pd.DataFrame
    preprocessing_metadata: Dict[str, Any]
    transformers: Dict[str, Any]
    processing_time: float
    original_shape: Tuple[int, int]
    final_shape: Tuple[int, int]


class DataPreprocessor:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    –†–µ–∞–ª–∏–∑—É–µ—Ç enterprise patterns
    """
    
    def __init__(self, config: Optional[AutoMLConfig] = None):
        self.config = config or AutoMLConfig()
        self.preprocessing_config = self.config.data_preprocessing
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.fitted_transformers = {}
        self.preprocessing_pipeline = None
        self.is_fitted = False
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.preprocessing_metadata = {}
        
        logger.info("üîß DataPreprocessor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def preprocess(
        self,
        data: pd.DataFrame,
        fit: bool = True,
        preserve_index: bool = True
    ) -> pd.DataFrame:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            fit: –û–±—É—á–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (True –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏)
            preserve_index: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω–¥–µ–∫—Å
        """
        import time
        start_time = time.time()
        
        logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {data.shape}")
        
        original_shape = data.shape
        processed_data = data.copy()
        
        try:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
            ) as progress:
                
                # –≠—Ç–∞–ø 1: –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
                task = progress.add_task("–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...", total=None)
                processed_data = self._basic_cleaning(processed_data)
                
                # –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                progress.update(task, description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
                processed_data = self._handle_missing_values(processed_data, fit)
                
                # –≠—Ç–∞–ø 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
                progress.update(task, description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤...")
                processed_data = self._handle_outliers(processed_data, fit)
                
                # –≠—Ç–∞–ø 4: –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                progress.update(task, description="–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
                processed_data = self._encode_categorical(processed_data, fit)
                
                # –≠—Ç–∞–ø 5: –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                progress.update(task, description="–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
                processed_data = self._scale_features(processed_data, fit)
                
                # –≠—Ç–∞–ø 6: –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
                progress.update(task, description="–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π...")
                processed_data = self._remove_low_variance_features(processed_data, fit)
                
                # –≠—Ç–∞–ø 7: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                progress.update(task, description="–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞...")
                processed_data = self._final_cleaning(processed_data)
                
                progress.update(task, description="‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", completed=True)
        
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            processing_time = time.time() - start_time
            final_shape = processed_data.shape
            
            self.preprocessing_metadata = {
                'original_shape': original_shape,
                'final_shape': final_shape,
                'processing_time': processing_time,
                'rows_removed': original_shape[0] - final_shape[0],
                'columns_removed': original_shape[1] - final_shape[1],
                'missing_values_handled': True,
                'outliers_handled': True,
                'categorical_encoded': True,
                'features_scaled': True
            }
            
            if fit:
                self.is_fitted = True
            
            logger.info(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {original_shape} ‚Üí {final_shape} –∑–∞ {processing_time:.2f}—Å")
            
            return processed_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return data  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    
    def preprocess_target(self, target: pd.Series, fit: bool = True) -> pd.Series:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        logger.info("üéØ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...")
        
        processed_target = target.copy()
        
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if processed_target.isna().any():
                if self.preprocessing_config.missing_value_strategy == 'drop':
                    processed_target = processed_target.dropna()
                else:
                    fill_value = processed_target.mean()
                    processed_target = processed_target.fillna(fill_value)
                    logger.info(f"üìù –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {target.isna().sum()} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            if self.preprocessing_config.outlier_handling != 'none':
                processed_target = self._handle_target_outliers(processed_target, fit)
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)
            if self.preprocessing_config.scale_target:
                processed_target = self._scale_target(processed_target, fit)
            
            logger.info(f"‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(target)} ‚Üí {len(processed_target)}")
            
            return processed_target
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {e}")
            return target
    
    def _basic_cleaning(self, data: pd.DataFrame) -> pd.DataFrame:
        """–ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üßπ –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        cleaned_data = data.copy()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –∫–æ–ª–æ–Ω–æ–∫
        initial_shape = cleaned_data.shape
        cleaned_data = cleaned_data.dropna(how='all', axis=0)  # –°—Ç—Ä–æ–∫–∏
        cleaned_data = cleaned_data.dropna(how='all', axis=1)  # –ö–æ–ª–æ–Ω–∫–∏
        
        if cleaned_data.shape != initial_shape:
            logger.info(f"üìù –£–¥–∞–ª–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏/–∫–æ–ª–æ–Ω–∫–∏: {initial_shape} ‚Üí {cleaned_data.shape}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫
        duplicates = cleaned_data.duplicated().sum()
        if duplicates > 0:
            cleaned_data = cleaned_data.drop_duplicates()
            logger.info(f"üìù –£–¥–∞–ª–µ–Ω–æ {duplicates} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç—Ä–æ–∫")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç (–∫–æ–ª–æ–Ω–∫–∏ —Å –æ–¥–Ω–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º)
        constant_columns = []
        for col in cleaned_data.columns:
            if cleaned_data[col].nunique() <= 1:
                constant_columns.append(col)
        
        if constant_columns:
            cleaned_data = cleaned_data.drop(columns=constant_columns)
            logger.info(f"üìù –£–¥–∞–ª–µ–Ω—ã –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {constant_columns}")
        
        return cleaned_data
    
    def _handle_missing_values(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        logger.info("üï≥Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")
        
        if not data.isna().any().any():
            logger.info("üìù –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
            return data
        
        strategy = self.preprocessing_config.missing_value_strategy
        threshold = self.preprocessing_config.missing_value_threshold
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing_ratios = data.isna().sum() / len(data)
        columns_to_drop = missing_ratios[missing_ratios > threshold].index.tolist()
        
        if columns_to_drop:
            data = data.drop(columns=columns_to_drop)
            logger.info(f"üìù –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ —Å >({threshold*100}%) –ø—Ä–æ–ø—É—Å–∫–æ–≤: {columns_to_drop}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_columns = data.select_dtypes(exclude=[np.number]).columns.tolist()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if numeric_columns:
            if strategy == 'mean':
                imputer_numeric = SimpleImputer(strategy='mean')
            elif strategy == 'median':
                imputer_numeric = SimpleImputer(strategy='median')
            elif strategy == 'forward_fill':
                data[numeric_columns] = data[numeric_columns].fillna(method='ffill')
                imputer_numeric = None
            else:  # KNN imputation
                imputer_numeric = KNNImputer(n_neighbors=5)
            
            if imputer_numeric and fit:
                data[numeric_columns] = imputer_numeric.fit_transform(data[numeric_columns])
                self.fitted_transformers['numeric_imputer'] = imputer_numeric
            elif imputer_numeric and not fit and 'numeric_imputer' in self.fitted_transformers:
                data[numeric_columns] = self.fitted_transformers['numeric_imputer'].transform(data[numeric_columns])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if categorical_columns:
            imputer_categorical = SimpleImputer(strategy='most_frequent')
            
            if fit:
                data[categorical_columns] = imputer_categorical.fit_transform(data[categorical_columns])
                self.fitted_transformers['categorical_imputer'] = imputer_categorical
            elif 'categorical_imputer' in self.fitted_transformers:
                data[categorical_columns] = self.fitted_transformers['categorical_imputer'].transform(data[categorical_columns])
        
        remaining_missing = data.isna().sum().sum()
        if remaining_missing > 0:
            logger.warning(f"‚ö†Ô∏è –û—Å—Ç–∞–ª–∏—Å—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {remaining_missing}")
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ - –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω—É–ª—è–º–∏
            data = data.fillna(0)
        else:
            logger.info("‚úÖ –í—Å–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
        
        return data
    
    def _handle_outliers(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤"""
        logger.info("üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤...")
        
        method = self.preprocessing_config.outlier_detection_method
        handling = self.preprocessing_config.outlier_handling
        threshold = self.preprocessing_config.outlier_threshold
        
        if handling == 'none':
            return data
        
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_columns:
            return data
        
        outliers_detected = 0
        
        for col in numeric_columns:
            try:
                if method == 'iqr':
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outliers_mask = (data[col] < lower_bound) | (data[col] > upper_bound)
                    
                elif method == 'zscore':
                    z_scores = np.abs(stats.zscore(data[col], nan_policy='omit'))
                    outliers_mask = z_scores > threshold
                    
                elif method == 'isolation_forest':
                    if fit:
                        iso_forest = IsolationForest(contamination=0.1, random_state=42)
                        outliers_pred = iso_forest.fit_predict(data[col].values.reshape(-1, 1))
                        self.fitted_transformers[f'isolation_forest_{col}'] = iso_forest
                    else:
                        if f'isolation_forest_{col}' in self.fitted_transformers:
                            iso_forest = self.fitted_transformers[f'isolation_forest_{col}']
                            outliers_pred = iso_forest.predict(data[col].values.reshape(-1, 1))
                        else:
                            continue
                    
                    outliers_mask = outliers_pred == -1
                
                outliers_count = outliers_mask.sum()
                if outliers_count > 0:
                    outliers_detected += outliers_count
                    
                    if handling == 'remove':
                        data = data[~outliers_mask]
                    elif handling == 'clip':
                        if method != 'isolation_forest':
                            data.loc[outliers_mask, col] = data[col].clip(lower_bound, upper_bound)
                        else:
                            # –î–ª—è isolation forest –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏
                            lower_clip = data[col].quantile(0.01)
                            upper_clip = data[col].quantile(0.99)
                            data.loc[outliers_mask, col] = data[col].clip(lower_clip, upper_clip)
                    elif handling == 'transform':
                        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                        if data[col].min() > 0:
                            data.loc[outliers_mask, col] = np.log1p(data.loc[outliers_mask, col])
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {e}")
                continue
        
        if outliers_detected > 0:
            logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {outliers_detected} –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º {method}")
        else:
            logger.info("üìù –í—ã–±—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
        
        return data
    
    def _encode_categorical(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("üî§ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        categorical_columns = data.select_dtypes(exclude=[np.number]).columns.tolist()
        
        if not categorical_columns:
            logger.info("üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")
            return data
        
        encoding_method = self.preprocessing_config.categorical_encoding
        max_categories = self.preprocessing_config.max_categories_onehot
        
        encoded_data = data.copy()
        
        for col in categorical_columns:
            try:
                unique_count = data[col].nunique()
                
                if encoding_method == 'onehot' and unique_count <= max_categories:
                    # One-Hot Encoding
                    if fit:
                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
                        encoded_features = encoder.fit_transform(data[[col]])
                        self.fitted_transformers[f'onehot_{col}'] = encoder
                    else:
                        if f'onehot_{col}' in self.fitted_transformers:
                            encoder = self.fitted_transformers[f'onehot_{col}']
                            encoded_features = encoder.transform(data[[col]])
                        else:
                            continue
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    feature_names = [f"{col}_{cat}" for cat in encoder.categories_[0]]
                    encoded_df = pd.DataFrame(encoded_features, columns=feature_names, index=data.index)
                    
                    # –ó–∞–º–µ–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
                    encoded_data = encoded_data.drop(columns=[col])
                    encoded_data = pd.concat([encoded_data, encoded_df], axis=1)
                    
                elif encoding_method == 'label' or unique_count > max_categories:
                    # Label Encoding
                    if fit:
                        encoder = LabelEncoder()
                        encoded_data[col] = encoder.fit_transform(data[col].astype(str))
                        self.fitted_transformers[f'label_{col}'] = encoder
                    else:
                        if f'label_{col}' in self.fitted_transformers:
                            encoder = self.fitted_transformers[f'label_{col}']
                            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                            try:
                                encoded_data[col] = encoder.transform(data[col].astype(str))
                            except ValueError:
                                # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º -1
                                encoded_values = []
                                for value in data[col].astype(str):
                                    if value in encoder.classes_:
                                        encoded_values.append(encoder.transform([value])[0])
                                    else:
                                        encoded_values.append(-1)
                                encoded_data[col] = encoded_values
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ {col}: {e}")
                continue
        
        logger.info(f"‚úÖ –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ {len(categorical_columns)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        return encoded_data
    
    def _scale_features(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        logger.info("‚öñÔ∏è –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_columns:
            logger.info("üìù –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return data
        
        scaling_method = self.preprocessing_config.scaling_method
        scaled_data = data.copy()
        
        try:
            if scaling_method == 'standard':
                scaler = StandardScaler()
            elif scaling_method == 'robust':
                scaler = RobustScaler()
            elif scaling_method == 'minmax':
                scaler = MinMaxScaler()
            elif scaling_method == 'quantile':
                scaler = QuantileUniformTransformer()
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {scaling_method}")
                return data
            
            if fit:
                scaled_data[numeric_columns] = scaler.fit_transform(data[numeric_columns])
                self.fitted_transformers['feature_scaler'] = scaler
            else:
                if 'feature_scaler' in self.fitted_transformers:
                    scaler = self.fitted_transformers['feature_scaler']
                    scaled_data[numeric_columns] = scaler.transform(data[numeric_columns])
                else:
                    logger.warning("‚ö†Ô∏è –°–∫–µ–π–ª–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            logger.info(f"‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã {len(numeric_columns)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ—Ç–æ–¥–æ–º {scaling_method}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return data
        
        return scaled_data
    
    def _remove_low_variance_features(self, data: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π"""
        logger.info("üìâ –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π...")
        
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_columns:
            return data
        
        threshold = self.preprocessing_config.variance_threshold
        
        try:
            if fit:
                variance_selector = VarianceThreshold(threshold=threshold)
                selected_features = variance_selector.fit_transform(data[numeric_columns])
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                selected_mask = variance_selector.get_support()
                selected_columns = [col for col, mask in zip(numeric_columns, selected_mask) if mask]
                removed_columns = [col for col, mask in zip(numeric_columns, selected_mask) if not mask]
                
                self.fitted_transformers['variance_selector'] = variance_selector
                self.fitted_transformers['selected_numeric_columns'] = selected_columns
            else:
                if 'selected_numeric_columns' in self.fitted_transformers:
                    selected_columns = self.fitted_transformers['selected_numeric_columns']
                    removed_columns = [col for col in numeric_columns if col not in selected_columns]
                else:
                    return data
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
            filtered_data = data.copy()
            if removed_columns:
                filtered_data = filtered_data.drop(columns=removed_columns)
                logger.info(f"üìù –£–¥–∞–ª–µ–Ω–æ {len(removed_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∏–∑–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π")
            else:
                logger.info("üìù –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é")
            
            return filtered_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –¥–∏—Å–ø–µ—Ä—Å–∏–∏: {e}")
            return data
    
    def _final_cleaning(self, data: pd.DataFrame) -> pd.DataFrame:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üèÅ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        cleaned_data = data.copy()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        infinite_mask = np.isinf(cleaned_data.select_dtypes(include=[np.number]))
        if infinite_mask.any().any():
            cleaned_data = cleaned_data.replace([np.inf, -np.inf], np.nan)
            cleaned_data = cleaned_data.fillna(0)
            logger.info("üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
        nan_count = cleaned_data.isna().sum().sum()
        if nan_count > 0:
            cleaned_data = cleaned_data.fillna(0)
            logger.info(f"üìù –ó–∞–ø–æ–ª–Ω–µ–Ω–æ {nan_count} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è NaN –∑–Ω–∞—á–µ–Ω–∏–π")
        
        return cleaned_data
    
    def _handle_target_outliers(self, target: pd.Series, fit: bool = True) -> pd.Series:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        method = self.preprocessing_config.outlier_detection_method
        threshold = self.preprocessing_config.outlier_threshold
        
        if method == 'iqr':
            Q1 = target.quantile(0.25)
            Q3 = target.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_mask = (target < lower_bound) | (target > upper_bound)
            
        elif method == 'zscore':
            z_scores = np.abs(stats.zscore(target, nan_policy='omit'))
            outliers_mask = z_scores > threshold
        
        else:
            return target
        
        outliers_count = outliers_mask.sum()
        if outliers_count > 0:
            # –û–±—Ä–µ–∑–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
            target_clipped = target.clip(target.quantile(0.01), target.quantile(0.99))
            logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {outliers_count} –≤—ã–±—Ä–æ—Å–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")
            return target_clipped
        
        return target
    
    def _scale_target(self, target: pd.Series, fit: bool = True) -> pd.Series:
        """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        try:
            if fit:
                scaler = StandardScaler()
                scaled_target = scaler.fit_transform(target.values.reshape(-1, 1)).flatten()
                self.fitted_transformers['target_scaler'] = scaler
            else:
                if 'target_scaler' in self.fitted_transformers:
                    scaler = self.fitted_transformers['target_scaler']
                    scaled_target = scaler.transform(target.values.reshape(-1, 1)).flatten()
                else:
                    return target
            
            return pd.Series(scaled_target, index=target.index)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: {e}")
            return target
    
    def inverse_transform_target(self, scaled_target: pd.Series) -> pd.Series:
        """–û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        if 'target_scaler' not in self.fitted_transformers:
            return scaled_target
        
        try:
            scaler = self.fitted_transformers['target_scaler']
            original_target = scaler.inverse_transform(scaled_target.values.reshape(-1, 1)).flatten()
            return pd.Series(original_target, index=scaled_target.index)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {e}")
            return scaled_target
    
    def save_transformers(self, filepath: Union[str, Path]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        transformers_data = {
            'fitted_transformers': self.fitted_transformers,
            'preprocessing_metadata': self.preprocessing_metadata,
            'is_fitted': self.is_fitted,
            'config': self.preprocessing_config.__dict__ if hasattr(self.preprocessing_config, '__dict__') else str(self.preprocessing_config)
        }
        
        joblib.dump(transformers_data, filepath)
        logger.info(f"üíæ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
    
    def load_transformers(self, filepath: Union[str, Path]):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"–§–∞–π–ª —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        
        transformers_data = joblib.load(filepath)
        
        self.fitted_transformers = transformers_data['fitted_transformers']
        self.preprocessing_metadata = transformers_data['preprocessing_metadata']
        self.is_fitted = transformers_data['is_fitted']
        
        logger.info(f"üìÇ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {filepath}")
    
    def get_preprocessing_report(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ"""
        if not self.preprocessing_metadata:
            return "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—â–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
        
        metadata = self.preprocessing_metadata
        
        report = f"""
=== –û–¢–ß–ï–¢ –ü–û –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ï –î–ê–ù–ù–´–• ===

–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {metadata.get('original_shape', 'N/A')}
–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {metadata.get('final_shape', 'N/A')}
–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {metadata.get('processing_time', 0):.2f}—Å

–ò–∑–º–µ–Ω–µ–Ω–∏—è:
- –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫: {metadata.get('rows_removed', 0)}
- –£–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {metadata.get('columns_removed', 0)}

–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã:
- –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {'‚úÖ' if metadata.get('missing_values_handled') else '‚ùå'}
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤: {'‚úÖ' if metadata.get('outliers_handled') else '‚ùå'}
- –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {'‚úÖ' if metadata.get('categorical_encoded') else '‚ùå'}
- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {'‚úÖ' if metadata.get('features_scaled') else '‚ùå'}

–û–±—É—á–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã: {len(self.fitted_transformers)}
"""
        
        return report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è DataPreprocessor
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    n_samples = 1000
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏
    data = pd.DataFrame({
        'numeric_normal': np.random.randn(n_samples),
        'numeric_with_outliers': np.concatenate([
            np.random.randn(n_samples - 50),
            np.random.randn(50) * 10  # –í—ã–±—Ä–æ—Å—ã
        ]),
        'numeric_with_missing': np.random.randn(n_samples),
        'categorical': np.random.choice(['A', 'B', 'C', 'D'], n_samples),
        'binary': np.random.choice([0, 1], n_samples),
        'constant': [1] * n_samples,  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
    })
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    missing_indices = np.random.choice(n_samples, size=100, replace=False)
    data.loc[missing_indices, 'numeric_with_missing'] = np.nan
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    target = pd.Series(
        data['numeric_normal'] * 2 + 
        data['binary'] * 3 + 
        np.random.randn(n_samples) * 0.5
    )
    
    print("=== –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï ===")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {data.isna().sum().sum()}")
    print(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n{data.dtypes}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    config = AutoMLConfig()
    preprocessor = DataPreprocessor(config)
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    processed_data = preprocessor.preprocess(data, fit=True)
    processed_target = preprocessor.preprocess_target(target)
    
    print("\n=== –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï ===")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {processed_data.shape}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {processed_data.isna().sum().sum()}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(processed_data.columns)}")
    
    # –û—Ç—á–µ—Ç –ø–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
    print(preprocessor.get_preprocessing_report())
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤)
    test_data = data.iloc[-100:].copy()
    processed_test_data = preprocessor.preprocess(test_data, fit=False)
    
    print(f"\n=== –¢–ï–°–¢–û–í–´–ï –î–ê–ù–ù–´–ï ===")
    print(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {test_data.shape}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞: {processed_test_data.shape}")